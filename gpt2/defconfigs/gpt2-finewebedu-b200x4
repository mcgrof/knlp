#
# GPT-2 Configuration for 4x NVIDIA B200 (192GB each, 768GB total)
# FineWebEdu dataset - Comparing AdamWSPAM vs baseline Adam at 50%
# sparsity:
#   1. AdamWSPAM + magnitude pruning @ 50%
#   2. Adam + magnitude pruning @ 50%
#
# Tests if SPAM spike detection and mitigation provides better
# training dynamics compared to baseline Adam optimizer when both
# use identical magnitude-based pruning at 50% sparsity target.
#
# Optimized for MAXIMUM SPEED with 4x B200 GPUs using DDP
#

# Model Selection - GPT-2
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL="gpt2"
CONFIG_TEST_MODELS="gpt2"
CONFIG_GPT2_MODEL_SIZE="124M"
# Enable model compilation for maximum performance
CONFIG_COMPILE_MODEL=y

# Enable test matrix mode for 2-test comparison
CONFIG_TEST_MATRIX_MODE=y

# Optimizer Configuration - Test AdamWSPAM vs baseline Adam
CONFIG_OPTIMIZER_MODE_MULTIPLE=y
CONFIG_OPTIMIZER_ENABLE_ADAMWSPAM=y
CONFIG_OPTIMIZER_ENABLE_ADAM=y
CONFIG_TEST_OPTIMIZER_ENABLED_ADAMWSPAM=y
CONFIG_TEST_OPTIMIZER_ENABLED_ADAM=y
# Disable all other optimizers
CONFIG_OPTIMIZER_ENABLE_SGD=n
CONFIG_OPTIMIZER_ENABLE_ADAMW=n
CONFIG_OPTIMIZER_ENABLE_ADAMWADV=n
CONFIG_OPTIMIZER_ENABLE_ADAMWPRUNE=n

# SPAM settings (used by AdamWSPAM)
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_PERIODIC_RESET=y

# Pruning Configuration - Both optimizers use magnitude pruning
# at 50% sparsity. This isolates the effect of SPAM spike
# detection by keeping pruning method constant.
CONFIG_PRUNING_MODE_MULTIPLE=y
CONFIG_PRUNING_ENABLE_MAGNITUDE=y
CONFIG_PRUNING_ENABLE_MOVEMENT=n
CONFIG_PRUNING_ENABLE_STATE=n
CONFIG_PRUNING_ENABLE_NONE=n
CONFIG_PRUNING_ENABLED_MAGNITUDE=y
CONFIG_TEST_PRUNING_MAGNITUDE=y
CONFIG_TEST_PRUNING_METHODS="magnitude"

# Sparsity - Only 50% for this comparison
CONFIG_TARGET_SPARSITY="0.5"
CONFIG_SPARSITY_ENABLE_50=y
CONFIG_SPARSITY_ENABLE_70=n
CONFIG_SPARSITY_ENABLE_90=n
CONFIG_SPARSITY_ENABLE_95=n
CONFIG_SPARSITY_ENABLE_99=n
CONFIG_TEST_SPARSITY_50=y
CONFIG_TEST_SPARSITY_70=n
CONFIG_TEST_SPARSITY_90=n

# Distributed Training - 4x B200 with DDP for MAXIMUM SPEED
CONFIG_GPT2_USE_DDP=y
CONFIG_GPT2_DDP_NUM_GPUS=4
CONFIG_GPT2_DDP_BACKEND="nccl"
CONFIG_GPT2_DDP_FIND_UNUSED_PARAMS=n

# Training Parameters - MASSIVE batch size for 768GB total VRAM
# Per-GPU batch size of 128=512 total across 4 GPUs
# With gradient accumulation of 2=effective batch size of 1024
CONFIG_BATCH_SIZE=128
CONFIG_GPT2_GRADIENT_ACCUMULATION=2
CONFIG_NUM_EPOCHS=10
CONFIG_GPT2_MAX_ITERS=50000
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10

# Learning Rate Schedule
CONFIG_LEARNING_RATE="6e-4"
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y

# Dataset Configuration
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024

# Data Loading Optimization - MAXIMUM THROUGHPUT
CONFIG_NUM_WORKERS=64
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=16

# Mixed Precision Training
CONFIG_MIXED_PRECISION=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Performance Optimizations - ALL THE BELLS AND WHISTLES
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPU_WARMUP=y
CONFIG_GPT2_FLASH_ATTENTION=y

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Checkpointing
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000
CONFIG_OUTPUT_DIR="./results/gpt2-adamwspam-vs-adam-b200x4"

# Experiment Tracking
CONFIG_GPT2_TRACKER="none"
