# R-MLP + Attention Injection + KV Pruning (W7900 Optimized)
#
# Focus: Attention-aware Reciprocal MLP with KV cache pruning
# Hardware: AMD Radeon Pro W7900 (48GB VRAM)
#
# Key innovation: R-MLP receives attention context via cheap vector add
#   Standard pathway: h_std = GELU(up_std(x))
#   Reciprocal pathway: h_low = GELU(up_low(x + α*attn))
#   NO extra GEMMs - same compute as baseline!
#
# Based on cowboy experiment findings:
# - RA alone degrades performance (12% worse than baseline)
# - R-MLP alone is nearly equivalent to baseline (0.26% worse)
# - R-MLP + KV pruning provides memory benefits with minimal quality loss
#
# Ablation steps (full cowboy: learned gates + learned KV pruning from step 0):
# V1: R-MLP baseline (R_ff=1152, 37.5%, golden ratio)
#     Geometric init: w_std=0.625, w_rec=0.375, α=1.0
# V2: R-MLP medium-large (R_ff=768, 25%) + weight tying
#     Tests explicit parameter sharing (up_low ↔ attn.c_proj)
#     R_ff=768 is max for weight tying (n_embd constraint)
#     Direct A/B test with V5 (same R_ff, different tying)
#     Geometric init: w_std=0.750, w_rec=0.250, α=1.0
# V3: R-MLP small (R_ff=256, 8.3%)
#     Geometric init: w_std=0.917, w_rec=0.083, α=1.0
# V4: R-MLP medium (R_ff=512, 16.7%)
#     Geometric init: w_std=0.833, w_rec=0.167, α=1.0
# V5: R-MLP medium-large (R_ff=768, 25%)
#     Same dimensions as V2, but NO weight tying
#     Geometric init: w_std=0.750, w_rec=0.250, α=1.0
# V6: R-MLP large (R_ff=1920, 62.5%, inverse golden)
#     Geometric init: w_std=0.375, w_rec=0.625, α=1.0
# V7: R-MLP baseline + gate-informed adaptive KV pruning
#     Tests feedback loop: R-MLP gates modulate pruning ratio
#     High w_rec×α (confidence) → prune more (R-MLP compensates)
#     Low w_rec×α (no compensation) → prune less (preserve quality)
#     Same R_ff=1152 as V1 for direct comparison
#     Geometric init: w_std=0.625, w_rec=0.375, α=1.0
#
# Alpha initialization rationale (α=1.0 for all steps):
# Empirical measurement shows ||attn|| / ||x|| ≈ 0.05 (5%), meaning
# attention signal is naturally ~5% of residual stream magnitude.
# Setting α=1.0 injects full attention at its natural scale, which
# is already low relative to x. This provides clean signal for
# reciprocal pathway to learn from without overwhelming the standard
# pathway. All steps track w_std, w_rec, and α evolution to evaluate
# learning dynamics and validate initialization choice.
#
# Baseline: Use BASELINE=mcgrof-citizen/gpt2-kv-pruning/6dvbpfuh from W&B
#
# Training time: 2 hours per step (can override with GPT2_MAX_TIME=)
#
# To run: make defconfig-gpt2-r-mlp-prune && make BASELINE=mcgrof-citizen/gpt2-kv-pruning/6dvbpfuh

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
# Time-based training (2 hours per step for quality convergence)
# For quick test, use: GPT2_MAX_TIME=600 make (10 minutes)
CONFIG_GPT2_MAX_TIME=7200
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=200
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=n
# CONFIG_COMPILE_MODEL is not set
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters - GPU-optimized for W7900 48GB
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled (using KV cache pruning instead)
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration - Disabled (focusing on R-MLP only)
# Note: RA showed 12% performance degradation in cowboy experiment
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=128
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.3"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
CONFIG_RA_MLA_USE_FLASH=n
CONFIG_RA_MLA_LOG_METRICS=n
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
CONFIG_RA_MLA_USE_ROPE=n

# Enable ablation study mode with R-MLP steps
# Skipping V0 (baseline already available from W&B)
# Skipping RA (V16) as it showed 12% degradation
# V1-V6: Various R-MLP + attention injection + KV pruning configurations
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="V1,V2,V3,V4,V5,V6,V7"

# Experiment tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-r-mlp-prune"

# Logging
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000
CONFIG_VERBOSE=y

# Test matrix
CONFIG_TEST_MATRIX_MODE=y
CONFIG_OUTPUT_DIR="test_matrix_results_r_mlp_prune"
