# GPT-2 with AdamWPrune on Shakespeare dataset
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_SHAKESPEARE=y
CONFIG_GPT2_DATASET_NAME="shakespeare"
CONFIG_GPT2_BLOCK_SIZE=256
# Time-based training (hardware-independent)
# 30 minutes for Shakespeare (small dataset)
CONFIG_GPT2_MAX_TIME=1800
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_WARMUP_STEPS=100
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=y
CONFIG_GPT2_COMPILE=y
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y

# Training parameters
CONFIG_NUM_EPOCHS=1
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=4
CONFIG_DEVICE="cuda"

# Optimizer
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWPRUNE=y
CONFIG_OPTIMIZER="adamwprune"
CONFIG_ADAMWPRUNE_BASE_OPTIMIZER_NAME="adamw"
CONFIG_ADAMWPRUNE_BETA1="0.9"
CONFIG_ADAMWPRUNE_BETA2="0.999"
CONFIG_ADAMWPRUNE_WEIGHT_DECAY="0.1"
CONFIG_ADAMWPRUNE_AMSGRAD=y

# Pruning - 50% state-based pruning for AdamWPrune
CONFIG_PRUNING_MODE_SINGLE=y
CONFIG_PRUNING_SELECT_STATE=y
CONFIG_PRUNING_METHOD="state"
CONFIG_TARGET_SPARSITY="0.5"  # 50% pruning target
CONFIG_PRUNING_WARMUP=1000
CONFIG_ADAMWPRUNE_ENABLE_PRUNING=y
CONFIG_ADAMWPRUNE_TARGET_SPARSITY="0.5"  # 50% pruning
CONFIG_ADAMWPRUNE_WARMUP_STEPS=1000
CONFIG_ADAMWPRUNE_FREQUENCY=50
CONFIG_ADAMWPRUNE_RAMP_END_EPOCH=1  # Faster ramp for GPT-2

# Advanced options
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
