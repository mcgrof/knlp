#
# GPT-2 Configuration for 4x NVIDIA B200 (192GB each, 768GB total)
# FineWebEdu dataset - Testing Bitter8 (Doom-style optimization)
#
# Bitter8: FP16 + Fast inverse sqrt (Doom hack)
# Same pruning metric as bitter7 but optimized for GPU memory bandwidth.
#
# DOOM-STYLE OPTIMIZATION:
#
# Memory bandwidth optimization:
# - FP16 precision for importance computation (2x bandwidth reduction)
# - Fast inverse sqrt: rsqrt(rsqrt(x)) = x^0.25 (faster than pow)
# - Same pruning quality as bitter7
#
# Expected benefits over bitter7:
# - 30-50% faster pruning due to FP16 and fast rsqrt
# - 2x less memory bandwidth (FP16 vs FP32)
# - Lower GPU memory access time
# - Higher compute utilization
#
# This validates the Doom hack approach on modern GPUs.
#

# Model Selection - GPT-2
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL="gpt2"
CONFIG_TEST_MODELS="gpt2"
CONFIG_GPT2_MODEL_SIZE="124M"
CONFIG_COMPILE_MODEL=y

# Single test mode (bitter8 only)
CONFIG_TEST_MATRIX_MODE=n

# Optimizer Configuration - AdamWPrune only
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWPRUNE=y
CONFIG_OPTIMIZER="adamwprune"

# SPAM settings (AdamWPrune base)
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_PERIODIC_RESET=y

# AdamWPrune specific settings
CONFIG_ADAMWPRUNE_BASE_ADAMWSPAM=y
CONFIG_ADAMWPRUNE_ENABLE_PRUNING=y
CONFIG_ADAMWPRUNE_TARGET_SPARSITY="0.5"
CONFIG_ADAMWPRUNE_FREQUENCY=100
CONFIG_ADAMWPRUNE_WARMUP_STEPS=1000
CONFIG_ADAMWPRUNE_RAMP_END_STEP=3000
CONFIG_ADAMWPRUNE_BETA1="0.9"
CONFIG_ADAMWPRUNE_BETA2="0.999"
CONFIG_ADAMWPRUNE_WEIGHT_DECAY="0.1"
CONFIG_ADAMWPRUNE_AMSGRAD=n

# AdamWPrune Variants - ONLY bitter8
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER0=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER1=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER2=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER3=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER4=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER5=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER6=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER7=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER8=y
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER9=n
CONFIG_GPT2_ADAMWPRUNE_DEFAULT_VARIANT="bitter8"

# Pruning Configuration - State pruning only
CONFIG_PRUNING_MODE_SINGLE=y
CONFIG_PRUNING_SELECT_STATE=y
CONFIG_PRUNING_METHOD="state"

# Sparsity - 50% target
CONFIG_TARGET_SPARSITY="0.5"
CONFIG_SPARSITY_ENABLE_50=y
CONFIG_SPARSITY_ENABLE_70=n
CONFIG_SPARSITY_ENABLE_90=n
CONFIG_SPARSITY_ENABLE_95=n
CONFIG_SPARSITY_ENABLE_99=n

# Distributed Training - 4x B200 with DDP
CONFIG_GPT2_USE_DDP=y
CONFIG_GPT2_DDP_NUM_GPUS=4
CONFIG_GPT2_DDP_BACKEND="nccl"
CONFIG_GPT2_DDP_FIND_UNUSED_PARAMS=n

# Training Parameters - Optimized for 4x B200 (768GB total VRAM)
CONFIG_BATCH_SIZE=128
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
CONFIG_NUM_EPOCHS=10
CONFIG_GPT2_MAX_ITERS=10000
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10

# Learning Rate Schedule
CONFIG_LEARNING_RATE="6e-4"
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y

# Dataset Configuration
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024

# Data Loading Optimization
CONFIG_NUM_WORKERS=16
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Mixed Precision Training
CONFIG_MIXED_PRECISION=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Performance Optimizations
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPU_WARMUP=y
CONFIG_GPT2_FLASH_ATTENTION=y

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Checkpointing
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000
CONFIG_OUTPUT_DIR="./results/gpt2-bitter8-fp16-b200x4"

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-bitter8-fp16-b200x4"
