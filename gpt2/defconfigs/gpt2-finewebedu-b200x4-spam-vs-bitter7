#
# GPT-2 Configuration for 4x NVIDIA B200 (192GB each, 768GB total)
# FineWebEdu dataset - Comparing pruning methods at 50% sparsity:
#   1. AdamWSPAM + magnitude pruning @ 50%
#   2. AdamWPrune + bitter7 variant @ 50%
#
# bitter7: Conservative variance-based pruning using
# |w| * (exp_avg_sq^0.25). Tests if variance accumulation
# (beta2=0.999) over ~1000 steps provides better pruning signal
# than standard magnitude pruning.
#
# Optimized for MAXIMUM SPEED with 4x B200 GPUs using DDP
#

# Model Selection - GPT-2
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL="gpt2"
CONFIG_TEST_MODELS="gpt2"
CONFIG_GPT2_MODEL_SIZE="124M"
# Enable model compilation for maximum performance
CONFIG_COMPILE_MODEL=y

# Enable test matrix mode for 2-test comparison
CONFIG_TEST_MATRIX_MODE=y

# Optimizer Configuration - Test AdamWSPAM and AdamWPrune
CONFIG_OPTIMIZER_MODE_MULTIPLE=y
CONFIG_OPTIMIZER_ENABLE_ADAMWSPAM=y
CONFIG_OPTIMIZER_ENABLE_ADAMWPRUNE=y
CONFIG_TEST_OPTIMIZER_ENABLED_ADAMWSPAM=y
CONFIG_TEST_OPTIMIZER_ENABLED_ADAMWPRUNE=y
# Disable all other optimizers
CONFIG_OPTIMIZER_ENABLE_SGD=n
CONFIG_OPTIMIZER_ENABLE_ADAM=n
CONFIG_OPTIMIZER_ENABLE_ADAMW=n
CONFIG_OPTIMIZER_ENABLE_ADAMWADV=n

# SPAM settings (used by both AdamWSPAM and AdamWPrune)
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_PERIODIC_RESET=y

# AdamWPrune specific settings
CONFIG_ADAMWPRUNE_BASE_ADAMWSPAM=y
CONFIG_ADAMWPRUNE_ENABLE_PRUNING=y
CONFIG_ADAMWPRUNE_TARGET_SPARSITY="0.5"
CONFIG_ADAMWPRUNE_FREQUENCY=100
CONFIG_ADAMWPRUNE_WARMUP_STEPS=1000
CONFIG_ADAMWPRUNE_BETA1="0.9"
CONFIG_ADAMWPRUNE_BETA2="0.999"
CONFIG_ADAMWPRUNE_WEIGHT_DECAY="0.1"
CONFIG_ADAMWPRUNE_AMSGRAD=n

# AdamWPrune Variants - Enable ONLY bitter7 for this test
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER0=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER1=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER2=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER3=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER4=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER5=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER6=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER7=y
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER8=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER9=n
CONFIG_GPT2_ADAMWPRUNE_DEFAULT_VARIANT="bitter7"

# Pruning Configuration - Test both magnitude and state at 50%
# AdamWSPAM will use magnitude pruning (standard |w| importance)
# AdamWPrune will use state pruning with bitter7 variant
CONFIG_PRUNING_MODE_MULTIPLE=y
CONFIG_PRUNING_ENABLE_MAGNITUDE=y
CONFIG_PRUNING_ENABLE_MOVEMENT=n
CONFIG_PRUNING_ENABLE_STATE=y
CONFIG_PRUNING_ENABLE_NONE=n
CONFIG_PRUNING_ENABLED_MAGNITUDE=y
CONFIG_PRUNING_ENABLED_STATE=y
CONFIG_TEST_PRUNING_MAGNITUDE=y
CONFIG_TEST_PRUNING_METHODS="magnitude"

# Sparsity - Only 50% for this comparison
CONFIG_TARGET_SPARSITY="0.5"
CONFIG_SPARSITY_ENABLE_50=y
CONFIG_SPARSITY_ENABLE_70=n
CONFIG_SPARSITY_ENABLE_90=n
CONFIG_SPARSITY_ENABLE_95=n
CONFIG_SPARSITY_ENABLE_99=n
CONFIG_TEST_SPARSITY_50=y
CONFIG_TEST_SPARSITY_70=n
CONFIG_TEST_SPARSITY_90=n

# Distributed Training - 4x B200 with DDP for MAXIMUM SPEED
CONFIG_GPT2_USE_DDP=y
CONFIG_GPT2_DDP_NUM_GPUS=4
CONFIG_GPT2_DDP_BACKEND="nccl"
CONFIG_GPT2_DDP_FIND_UNUSED_PARAMS=n

# Training Parameters - MASSIVE batch size for 768GB total VRAM
# Per-GPU batch size of 128=512 total across 4 GPUs
# With gradient accumulation of 2=effective batch size of 1024
CONFIG_BATCH_SIZE=128
CONFIG_GPT2_GRADIENT_ACCUMULATION=2
CONFIG_NUM_EPOCHS=10
CONFIG_GPT2_MAX_ITERS=50000
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10

# Learning Rate Schedule
CONFIG_LEARNING_RATE="6e-4"
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y

# Dataset Configuration
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024

# Data Loading Optimization - MAXIMUM THROUGHPUT
CONFIG_NUM_WORKERS=64
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=16

# Mixed Precision Training
CONFIG_MIXED_PRECISION=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Performance Optimizations - ALL THE BELLS AND WHISTLES
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPU_WARMUP=y
CONFIG_GPT2_FLASH_ATTENTION=y

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Checkpointing
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000
CONFIG_OUTPUT_DIR="./results/gpt2-spam-vs-bitter7-b200x4"

# Experiment Tracking
CONFIG_GPT2_TRACKER="none"
