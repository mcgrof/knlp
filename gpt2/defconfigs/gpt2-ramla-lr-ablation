# GPT-2 RA-MLA Learning Rate Ablation
#
# Tests whether RA/MLA architectures enable more aggressive learning rates.
# Hypothesis: adaptive compute allocation provides implicit regularization,
# allowing higher LR without destabilizing.
#
# Architecture key:
#   B     = Baseline GPT-2 (standard attention)
#   MLA   = Multi-head Latent Attention (TL-cache compression)
#   RA    = Reciprocal Attention routing (current GPT-2 RA)
#   RAMLA = RA + MLA combined
#   RAMLAKV = RA + MLA + learned KVSplice compression
#
# Learning rate suffix:
#   0 = standard LR (6e-4)
#   1 = aggressive LR (1.2e-3)
#
# Steps:
#   B0, B1       - Baseline at standard/aggressive LR
#   MLA0, MLA1   - MLA at standard/aggressive LR
#   RA0, RA1     - RA routing at standard/aggressive LR
#   RAMLA0, RAMLA1 - RA+MLA at standard/aggressive LR
#   RAMLAKV0, RAMLAKV1 - Full stack at standard/aggressive LR

CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Model Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_N_LAYER=12
CONFIG_GPT2_N_HEAD=12
CONFIG_GPT2_N_EMBD=768

# Training time: 1 hour per step (3600 seconds)
CONFIG_GPT2_MAX_TIME=3600
CONFIG_GPT2_GRADIENT_ACCUMULATION=4
CONFIG_GPT2_EVAL_INTERVAL=50
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=y
CONFIG_GPT2_COMPILE=n
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Batch configuration
CONFIG_BATCH_SIZE=16
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA Configuration (for RA/RAMLA/RAMLAKV steps)
CONFIG_ENABLE_RA=y
CONFIG_RA_HEAD_FRAC="0.25"
CONFIG_RA_ROUTER_HIDDEN=16
CONFIG_RA_ROUTER_BIAS_FULL="-1.0"
CONFIG_RA_WARMUP_LOSS_DROP="0.05"
CONFIG_RA_COMPUTE_PENALTY_WEIGHT="0.01"

# MLA Configuration (for MLA/RAMLA/RAMLAKV steps)
CONFIG_MLA_D_LATENT=256
CONFIG_MLA_COMPRESSION_RATIO="0.5"

# Ablation mode
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="B0,B1,MLA0,MLA1,RA0,RA1,RAMLA0,RAMLA1,RAMLAKV0,RAMLAKV1"

# Inference benchmarks
CONFIG_INFERENCE_BENCHMARK=y
CONFIG_RUN_LM_EVAL=y
CONFIG_LM_EVAL_TASKS="hellaswag"
CONFIG_LM_EVAL_LIMIT=100

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-ramla-lr-ablation"

# Advanced options
CONFIG_COMPILE_MODEL=n
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
