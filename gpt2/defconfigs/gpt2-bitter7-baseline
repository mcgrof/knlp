#
# GPT-2 bitter7 single-run configuration with BASELINE support
# For 4x NVIDIA B200 GPUs (192GB each, 768GB total)
#
# Runs ONLY bitter7 with ability to reference a baseline run
# for comparison via BASELINE= parameter.
#
# Usage:
#   make defconfig-gpt2-bitter7-baseline
#   make BASELINE=mcgrof-citizen/gpt2-spam-vs-bitter7-b200x4/run_id
#
# This creates a NEW W&B project (gpt2-bitter7-b200x4) while
# fetching baseline metrics from the specified run for comparison.
#

# Model Selection - GPT-2
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL="gpt2"
CONFIG_TEST_MODELS="gpt2"
CONFIG_GPT2_MODEL_SIZE="124M"
CONFIG_COMPILE_MODEL=y

# Single optimizer mode - AdamWPrune only
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWPRUNE=y
CONFIG_OPTIMIZER="adamwprune"

# SPAM settings (used by AdamWPrune base)
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_PERIODIC_RESET=y

# AdamWPrune settings - bitter7 variant
CONFIG_ADAMWPRUNE_BASE_ADAMWSPAM=y
CONFIG_ADAMWPRUNE_ENABLE_PRUNING=y
CONFIG_ADAMWPRUNE_TARGET_SPARSITY="0.5"
CONFIG_ADAMWPRUNE_FREQUENCY=100
CONFIG_ADAMWPRUNE_WARMUP_STEPS=1000
CONFIG_ADAMWPRUNE_BETA1="0.9"
CONFIG_ADAMWPRUNE_BETA2="0.999"
CONFIG_ADAMWPRUNE_WEIGHT_DECAY="0.1"
CONFIG_ADAMWPRUNE_AMSGRAD=n

# Bitter7 variant selection
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER0=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER7=y

# Pruning - state pruning with bitter7
CONFIG_PRUNING_MODE_SINGLE=y
CONFIG_PRUNING_SELECT_STATE=y
CONFIG_PRUNING_METHOD="state"
CONFIG_TARGET_SPARSITY="0.5"
CONFIG_PRUNING_WARMUP=100

# Distributed Training - 4x B200 with DDP
CONFIG_GPT2_USE_DDP=y
CONFIG_GPT2_DDP_NUM_GPUS=4
CONFIG_GPT2_DDP_BACKEND="nccl"
CONFIG_GPT2_DDP_FIND_UNUSED_PARAMS=n

# Training Parameters - Same as SPAM vs bitter7 for fair comparison
CONFIG_BATCH_SIZE=128
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
CONFIG_NUM_EPOCHS=10
CONFIG_GPT2_MAX_TIME=14400
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10

# Learning Rate Schedule
CONFIG_LEARNING_RATE="6e-4"
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y

# Dataset Configuration
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024

# Data Loading Optimization
CONFIG_NUM_WORKERS=16
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Mixed Precision Training
CONFIG_MIXED_PRECISION=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Performance Optimizations
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPU_WARMUP=y
CONFIG_GPT2_FLASH_ATTENTION=y

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Checkpointing
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000
CONFIG_OUTPUT_DIR="./results/gpt2-bitter7-b200x4"

# Experiment Tracking - NEW PROJECT NAME
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-bitter7-b200x4"

# BASELINE support - set via make BASELINE=entity/project/run_id
# This will be populated automatically when using BASELINE= parameter
