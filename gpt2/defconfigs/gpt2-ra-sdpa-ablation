# GPT-2 RA vs SDPA Gating Ablation Study
#
# Tests attention mechanism variations:
#   baseline   - Standard GPT-2 (no KNLP features)
#   sdpa_gate  - SDPA output gating only (Qwen3-style)
#   ra         - Reciprocal Attention only
#
# This ablation compares:
# - Qwen3-style SDPA output gating: y = attn_output * sigmoid(W_gate @ x)
# - Reciprocal Attention: y = y_base + beta * ra_ln(SDPA(k, q, v))
#
# Features:
# - CONFIG_HYPER_PARAM_AUTO: Auto-detects batch size/gradient accumulation
# - FineWebEdu dataset for meaningful training data
# - Tracks RA beta values and gate statistics in W&B
#

CONFIG_HYPER_PARAM_AUTO=y
CONFIG_COMPILE_AUTO=n
CONFIG_GPT2_COMPILE=n

CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Model Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_N_LAYER=12
CONFIG_GPT2_N_HEAD=12
CONFIG_GPT2_N_EMBD=768

# Training time: 2 hours per test (7200 seconds)
CONFIG_GPT2_MAX_TIME=7200
CONFIG_GPT2_EVAL_INTERVAL=50
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=n
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Batch configuration
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# GPT2_KNLP Configuration
# Tests three variants automatically:
#   baseline   - Standard GPT-2 (no KNLP features enabled)
#   sdpa_gate  - SDPA output gating (Qwen3-style)
#   ra         - Reciprocal Attention (K@Q.T swap)
CONFIG_GPT2_KNLP=y
CONFIG_KNLP_VARIANT="baseline,sdpa_gate,ra"
# RA settings (used when ra variant is selected)
CONFIG_GPT2_KNLP_RA_LAYERS=3
CONFIG_GPT2_KNLP_RA_HEADS=1

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-ra-sdpa-ablation"

# Advanced options
CONFIG_COMPILE_MODEL=n
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Distributed Training (multi-GPU)
CONFIG_GPT2_USE_DDP=y
CONFIG_GPT2_DDP_FIND_UNUSED_PARAMS=y

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
