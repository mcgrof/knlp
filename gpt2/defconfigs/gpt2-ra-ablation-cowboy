# RA + R-MLP: Cowboy Mode (No Coupling Warmup)
#
# Tests geometric initialization without variance-guided coupling delays.
# Based on empirical discovery that gates should initialize to dimensional
# ratios: w_std = D_std/D, w_rec = R/D (sums to 1.0).
#
# For RA (D=64, R=4): w_std=0.9375, w_rec=0.0625
# For R-MLP (D_ff=3072, R_ff=64): w_std=0.9792, w_rec=0.0208
#
# This "cowboy" approach relies on geometric initialization to avoid early
# disruption, eliminating the need for coupling warmup or variance-guided
# activation delays.
#
# To run: make defconfig-gpt2-ra-ablation-cowboy && make
#
# Training modes:
# 1. Time-based (recommended): Set CONFIG_GPT2_MAX_TIME for fixed duration
#    - 2 hours:  CONFIG_GPT2_MAX_TIME=7200
#    - 8 hours:  CONFIG_GPT2_MAX_TIME=28800
#    - Override: GPT2_MAX_TIME=7200 make
# 2. Iteration-based: Set CONFIG_GPT2_MAX_ITERS (traditional)

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
# Time-based training (2 hours per step for quality convergence)
# For quick test, use: GPT2_MAX_TIME=600 make (10 minutes)
CONFIG_GPT2_MAX_TIME=7200
# Iteration-based alternative (uncomment to use instead of time-based)
# CONFIG_GPT2_MAX_ITERS=10400
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=200
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=y
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters - GPU-optimized for A10G 24GB
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration - Disabled (Unified RA uses different mechanism)
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=128
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.3"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
CONFIG_RA_MLA_USE_FLASH=n
CONFIG_RA_MLA_LOG_METRICS=n
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
CONFIG_RA_MLA_USE_ROPE=n

# Unified RA gate configuration
# Using per-head gates: separate w_std/w_rec for each attention head
# Testing showed per-layer gates are 1.6% faster but 26-45% worse quality
# Per-head gates preserve representational capacity for GPT-2's head diversity
CONFIG_RA_V5_PER_HEAD_GATES=y

# Enable ablation study mode with Unified RA + R-MLP steps
# Cowboy mode: Tests reciprocal mechanisms with geometric initialization
# V0: Baseline (no RA/R-MLP)
# V16: RA only (R=4, per-head gates)
# V17: R-MLP (R_ff=64) + KV pruning (fixed k=391)
# V18: R-MLP (R_ff=1152, golden ratio) + KV pruning (learned ratio)
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="V0,V16,V17,V18"

# Experiment tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-kv-pruning"

# Logging
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000
CONFIG_VERBOSE=y

# Test matrix
CONFIG_TEST_MATRIX_MODE=y
CONFIG_OUTPUT_DIR="test_matrix_results_ra_cowboy"
