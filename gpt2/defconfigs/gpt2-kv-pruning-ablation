# GPT-2 KV Cache Pruning Ablation Study
# Tests delayed activation + KV cache pruning strategies
#
# Hardware: 4× NVIDIA A10G (24GB each) with DDP
# Research: Does delayed activation help? Can we prune KV cache without quality loss?
#
# To run: make defconfig-gpt2-kv-pruning-ablation && make
#
# Training modes:
# 1. Time-based (recommended): Set CONFIG_GPT2_MAX_TIME for fixed duration
#    - 2 hours:  CONFIG_GPT2_MAX_TIME=7200
#    - 8 hours:  CONFIG_GPT2_MAX_TIME=28800
#    - Override: GPT2_MAX_TIME=7200 make
# 2. Iteration-based: Set CONFIG_GPT2_MAX_ITERS (traditional)

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
# Time-based training (2 hours per step for quality convergence)
# For quick test, use: GPT2_MAX_TIME=600 make (10 minutes)
CONFIG_GPT2_MAX_TIME=7200
# Iteration-based alternative (uncomment to use instead of time-based)
# CONFIG_GPT2_MAX_ITERS=10400
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=200
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=n
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters - GPU-optimized for A10G 24GB
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration - Disabled (using ablation mode instead)
CONFIG_ENABLE_RA_MLA=n

# Enable ablation study mode with KV pruning + variance-guided activation tests
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="V0,V16,V17,V18"

# Disable test matrix mode - ablation mode runs its own test sequence
CONFIG_TEST_MATRIX_MODE=n

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=n
CONFIG_TRACKER_PROJECT="gpt2-kv-pruning"

# Advanced options
# CONFIG_COMPILE_MODEL is not set
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Test execution
CONFIG_TEST_PARALLEL_JOBS=1
CONFIG_OUTPUT_DIR="test_matrix_results_kv_pruning"

# Ablation step details:
#
# BASELINE - Step V0
# ==================
# - Step V0: Pure baseline GPT-2
#   Architecture: Standard SDPA attention, standard MLP
#   Purpose: Baseline reference for all comparisons
#
# NEW ARCHITECTURE WITH VARIANCE-GUIDED ACTIVATION - Steps V16-V18
# =================================================================
# All steps use RA + R-MLP from ra.py with variance-guided
# activation instead of old MLA-based RA. Reciprocal pathways remain
# disabled until loss variance drops below threshold, then ramp 0→1
# over 200 steps. This data-driven approach prevents premature
# activation and MLP collapse.
#
# Variance-guided parameters:
#   - Check interval: every 50 steps
#   - Minimum step: 250 (earliest possible activation)
#   - Window size: 100 samples
#   - Threshold: 0.02 (variance must be < 0.02 to activate)
#   - Warmup: 200 steps after activation
#
# - Step V16: RA (R=4) with per-head gates + variance-guided
#   Architecture: ReciprocalAttention with learned per-head w_std/w_rec
#   Activation: Variance-guided (delayed until loss stabilizes)
#   Coupling: Ramps 0→1 over 200 steps after activation
#   Purpose: Test RA with learned gate ratios and delayed activation
#
# - Step V17: R-MLP basic (R_ff=64) + KV pruning + variance-guided
#   Architecture: ReciprocalMLP from ra.py (NEW, not old MLA-based!)
#   KV pruning: Fixed golden ratio k=391 (38.2% of cache kept)
#   Activation: Variance-guided (delayed until loss stabilizes)
#   Coupling: Ramps R-MLP pathways 0→1 over 200 steps after activation
#   Purpose: NEW R-MLP architecture + memory reduction + delayed start
#
# - Step V18: R-MLP golden (R_ff=1152) + learned KV pruning + variance
#   Architecture: ReciprocalMLP with golden ratio split
#   KV pruning: LEARNED keep_ratio (init 0.382, adapts during training)
#   Activation: Variance-guided (delayed until loss stabilizes)
#   Coupling: Ramps R-MLP pathways 0→1 over 200 steps after activation
#   Purpose: Large R-MLP + adaptive pruning + delayed activation
#
# Expected comparisons:
# - V16 vs V0: Does variance-guided RA improve over baseline?
# - V17 vs V16: Does R-MLP + KV pruning improve over RA alone?
# - V18 vs V17: Does golden ratio + learned pruning improve quality?
#
# Usage:
# 1. Full ablation (2 hours/step, 8 hours total for 4 steps):
#    make defconfig-gpt2-kv-pruning-ablation && make
#
# 2. Quick sanity check (60 seconds/step):
#    make defconfig-gpt2-kv-pruning-ablation
#    GPT2_MAX_TIME=60 make
#
# 3. Dry-run validation (CPU, ~20 seconds total):
#    make defconfig-gpt2-kv-pruning-ablation
#    make check
