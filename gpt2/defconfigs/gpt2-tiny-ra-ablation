#
# GPT-2 Tiny RA Ablation Study
#
# Compares baseline GPT-2 Tiny vs RA (double attention).
# Both use same architecture (6L/512d/8H), only difference is single vs double attention.
#

# Auto hyperparameters
CONFIG_HYPER_PARAM_AUTO=y

# Test matrix mode - run both baseline and RA
CONFIG_TEST_MATRIX_MODE=y

# Model configurations to test
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_GPT2_MODEL_NAME="gpt2-tiny"

# Dataset
CONFIG_GPT2_DATASET_TINYSTORIES=y
CONFIG_GPT2_BLOCK_SIZE=1024

# Training
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_MAX_ITERS=10000
CONFIG_GPT2_MAX_TIME=14400
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"

# Reproducibility
CONFIG_DOUBLE_ATTENTION_SEED=42

# Evaluation
CONFIG_INFERENCE_BENCHMARK=y
CONFIG_RUN_LM_EVAL=y
CONFIG_LM_EVAL_TASKS="hellaswag,arc_easy,winogrande"
CONFIG_LM_EVAL_LIMIT=100

# Optimizer
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_ADV_WEIGHT_DECAY="0.01"
CONFIG_ADV_USE_AMSGRAD=y

# Advanced
CONFIG_GPT2_FLASH_ATTENTION=y
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-tiny-ra-ablation"
CONFIG_AUTO_GENERATE_GRAPHS=y

# Test steps: baseline and RA
CONFIG_RA_ABLATION_STEPS="baseline,ra"
