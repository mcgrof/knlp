# RA with Learned Skip Gates Ablation
# Tests Reciprocal Attention with conditional computation
#
# Hardware: AMD Radeon Pro W7900 (48GB) or 4Ã— NVIDIA A10G (24GB)
# Research: Do learned skip gates solve RA degradation problem?
#
# Hypothesis: RA degraded 12% because forced to use both pathways.
# Skip gates let model disable unhelpful pathway per layer.
#
# To run: make defconfig-gpt2-ra-ablation && make

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
CONFIG_GPT2_MAX_TIME=7200
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=200
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=n
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=128
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.3"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
CONFIG_RA_MLA_USE_FLASH=n
CONFIG_RA_MLA_LOG_METRICS=n
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
CONFIG_RA_MLA_USE_ROPE=n

# RA gate configuration (per-head gates for quality)
CONFIG_RA_V5_PER_HEAD_GATES=y

# Enable ablation study mode with RA skip gates
# R0: Baseline (standard GPT-2)
# R1: RA with learned skip gates (R=4, geometric init)
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="R0,R1"

# Experiment tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-ra-skip"

# Logging
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000
CONFIG_VERBOSE=y

# Test matrix
CONFIG_TEST_MATRIX_MODE=y
CONFIG_OUTPUT_DIR="test_matrix_results_ra_skip"

# Ablation step details:
#
# R0: Baseline GPT-2
# =================
# Standard SDPA attention, standard MLP (3072)
# Purpose: Same-hardware baseline for RA comparison
#
# R1: RA with Learned Skip Gates
# ===============================
# Architecture: Reciprocal Attention (R=4, low-rank)
# Skip gates: skip_std_attn, skip_lowrank_attn (init 2.0, enabled)
# Geometric init: w_std=0.9375 (60/64), w_rec=0.0625 (4/64)
# Purpose: Test if skip gates solve RA degradation (was 12% worse)
#
# Skip gates enable model to:
# - Disable std attention where low-rank suffices (early layers?)
# - Disable low-rank where full-rank needed (late layers?)
# - Mix adaptively per layer (learned, not designed)
#
# Expected: RA with skip gates BEATS baseline by eliminating
# forced computation of unhelpful pathways.
#
# Key metrics tracked:
# - skip_std_attn, skip_lowrank_attn: Logits for each pathway
# - use_std_attn, use_lowrank_attn: Binary decisions (sigmoid > 0.5)
# - w_std, w_rec: Pathway weights (geometric init)
# - Perplexity: Final quality vs baseline
#
# Training time: 2 hours per step (4 hours total for R0+R1)
#
# Usage:
# make defconfig-gpt2-ra-ablation && make
# Quick test: GPT2_MAX_TIME=600 make (10 minutes/step)
# Dry-run: make check (~10 seconds)
