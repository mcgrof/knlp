# RA + R-MLP: Complete Ablation Study
# Tests Reciprocal Attention (V0-V2) and Reciprocal MLP (V3-V6)
#
# Hardware: 4× NVIDIA A10G (24GB each) with DDP
# Research: Does reciprocal folding improve quality at matched speed?
#
# To run: make defconfig-gpt2-ra-ablation && make
#
# Training modes:
# 1. Time-based (recommended): Set CONFIG_GPT2_MAX_TIME for fixed duration
#    - 2 hours:  CONFIG_GPT2_MAX_TIME=7200
#    - 8 hours:  CONFIG_GPT2_MAX_TIME=28800
#    - Override: GPT2_MAX_TIME=7200 make
# 2. Iteration-based: Set CONFIG_GPT2_MAX_ITERS (traditional)

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
# Time-based training (2 hours per step for quality convergence)
# For quick test, use: GPT2_MAX_TIME=600 make (10 minutes)
CONFIG_GPT2_MAX_TIME=7200
# Iteration-based alternative (uncomment to use instead of time-based)
# CONFIG_GPT2_MAX_ITERS=10400
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=200
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=y
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters - GPU-optimized for A10G 24GB
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration - Disabled (Unified RA uses different mechanism)
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=128
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.3"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
CONFIG_RA_MLA_USE_FLASH=n
CONFIG_RA_MLA_LOG_METRICS=n
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
CONFIG_RA_MLA_USE_ROPE=n

# Unified RA gate configuration
# Using per-head gates: separate w_std/w_rec for each attention head
# Testing showed per-layer gates are 1.6% faster but 26-45% worse quality
# Per-head gates preserve representational capacity for GPT-2's head diversity
CONFIG_RA_V5_PER_HEAD_GATES=y

# Enable ablation study mode with Unified RA + R-MLP steps
# Focus on most promising configurations based on 5-minute test results
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="V0,V1,V3,V7,V9,V16"

# Disable test matrix mode - ablation mode runs its own test sequence
CONFIG_TEST_MATRIX_MODE=n

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=n
CONFIG_TRACKER_PROJECT="gpt2-ra"

# Advanced options
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Test execution
CONFIG_TEST_PARALLEL_JOBS=1
CONFIG_OUTPUT_DIR="test_matrix_results_ra"

# Ablation step details:
#
# RECIPROCAL ATTENTION (RA) - Steps V0-V2
# ========================================
# - Step V0: Baseline GPT-2 (standard SDPA)
#   Speed: 1.33ms (eager), ~1.15ms (compiled)
#   Architecture: Standard Q,K,V projection + SDPA
#
# - Step V1: Unified RA (R=4, folded layout)
#   Speed: 1.33ms eager (MATCHES baseline!), ~1.15ms compiled
#   Architecture: Folded [Qf|Kf] emission, learned gates (w_std, w_rec)
#   Key: Split head dim D = D_std + R, fold reciprocal in single SDPA
#
# - Step V2: Unified RA + Self-Restart
#   Architecture: V1 + identity path: out = (1-α)·attn + α·V
#   Purpose: Test if self-restart stabilizes training
#
# RECIPROCAL MLP (R-MLP) - Steps V3-V6
# ======================================
# All build on Unified RA (V1) as attention foundation
#
# - Step V3: Unified RA + R-MLP (basic, R_ff=64)
#   Architecture: Folded MLP [h_std|h_low], gates (w_std, w_rec)
#   Key: Split expansion D_ff = D_ff_std + R_ff, FLOPs match baseline
#
# - Step V4: V3 + 1x1 mixer on h_low
#   Purpose: Test if mixer enhances low-rank features
#
# - Step V5: V3 + per-token gates (discoverability)
#   Purpose: Test adaptive scaling of reciprocal features
#
# - Step V6: V3 + mixer + gates (all mechanisms)
#   Purpose: Test composition of R-MLP features
#
# RA PARAMETER SENSITIVITY - Steps V7-V10
# ========================================
# Test Unified RA with different hyperparameters
#
# - Step V7: Unified RA with R=8 (higher reciprocal rank)
#   Purpose: Test if doubling reciprocal capacity improves quality
#
# - Step V8: Unified RA with R=8 + Self-Restart
#   Purpose: Test high reciprocal rank with stabilization
#
# - Step V9: Unified RA with R=2 (minimal reciprocal rank)
#   Purpose: Test if minimal reciprocal capacity is sufficient
#
# - Step V10: Unified RA (R=4) + Self-Restart + 6x MLP expansion
#   Purpose: Test if wider MLP (6x vs 4x) compensates for RA
#
# - Step V11: R-MLP basic with delayed activation (75 steps)
#   Architecture: R-MLP (R_ff=64) delayed start at step 75
#   Purpose: Simple R-MLP baseline with warmup
#
# - Step V12: Unified RA (R=4) with delayed activation (75 steps)
#   Architecture: Baseline GPT-2 for first 75 steps, then Unified RA
#   Purpose: Test if early training w_rec drop indicates RA hurts warmup
#   Based on: ra-mean.png observation (w_rec drops before step 75)
#
# - Step V13: R-MLP golden ratio with delayed activation (75 steps)
#   Architecture: Baseline MLP for first 75 steps, then R-MLP (R_ff=1152)
#   Purpose: Test if golden ratio R-MLP benefits from delayed start
#
# - Step V14: V11 + KV cache pruning (golden ratio)
#   Architecture: R-MLP basic delayed + KV pruning (k=391, 38.2% kept)
#   Purpose: Test KV cache reduction via top-k + recency pruning
#
# - Step V15: V13 + KV cache pruning (learned ratio)
#   Architecture: R-MLP golden delayed + learned KV pruning ratio
#   Purpose: Learn optimal KV pruning vs fixed golden ratio
#
# - Step V16: Unified RA + R-MLP with weight tying
#   Architecture: V3 (basic R-MLP) but with tie_up_low=True
#   Purpose: Test if tying up_low to transposed up_std subset reduces
#            parameters (~49K/layer saved) without quality loss
#   Params saved: 64 × 768 × 12 layers ≈ 590K parameters
#
# Usage:
# 1. Full ablation (2 hours/step, 22 hours total):
#    make defconfig-gpt2-ra-ablation && make
#
# 2. Quick sanity check (60 seconds/step):
#    make defconfig-gpt2-ra-ablation
#    GPT2_MAX_TIME=60 make
#
# 3. Dry-run validation (CPU, ~60 seconds total):
#    make defconfig-gpt2-ra-ablation
#    make check
