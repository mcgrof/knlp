# RA with Learned Skip Gates Ablation
# Tests Reciprocal Attention with conditional computation
#
# Hardware: AMD Radeon Pro W7900 (48GB) or 4× NVIDIA A10G (24GB)
# Research: Do learned skip gates solve RA degradation problem?
#
# Hypothesis: RA degraded 12% because forced to use both pathways.
# Skip gates let model disable unhelpful pathway per layer.
#
# To run: make defconfig-gpt2-ra-ablation && make

# Hyperparameter mode: manual (don't auto-tune for GPU)
CONFIG_HYPER_PARAM_MANUAL=y

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
CONFIG_GPT2_MAX_TIME=7200
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=200
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=n
CONFIG_COMPILE_MANUAL=y
# CONFIG_COMPILE_MODEL is not set
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=128
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.3"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
CONFIG_RA_MLA_USE_FLASH=n
CONFIG_RA_MLA_LOG_METRICS=n
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
CONFIG_RA_MLA_USE_ROPE=n

# RA gate configuration (per-head gates for quality)
CONFIG_RA_V5_PER_HEAD_GATES=y

# Enable ablation study mode with RA skip gates
# R0: Baseline (standard GPT-2) - already completed
# R1: RA with learned skip gates (R=4, aggressive init)
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="R1"

# Experiment tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-ra-skip"

# Logging
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000
CONFIG_VERBOSE=y

# Test matrix
CONFIG_TEST_MATRIX_MODE=y
CONFIG_OUTPUT_DIR="test_matrix_results_ra_skip"

# Ablation step details:
#
# R0: Baseline GPT-2
# =================
# Standard SDPA attention, standard MLP (3072)
# Purpose: Same-hardware baseline for RA comparison
#
# R1: RA with Aggressive Skip Gate Initialization
# =================================================
# Architecture: Reciprocal Attention (R=4, low-rank)
# Skip gates (ASYMMETRIC initialization):
#   skip_std_attn: init 2.0 (sigmoid≈0.88, ENABLED - proven to work)
#   skip_lowrank_attn: init -3.0 (sigmoid≈0.05, DISABLED - must prove value)
# Geometric pathway weights: w_std=0.9375 (60/64), w_rec=0.0625 (4/64)
# Purpose: Test if aggressive init prevents RA degradation
#
# Rationale for asymmetric initialization:
# - Previous R1 (symmetric init 2.0) degraded 32% vs baseline (49 vs 37 ppl)
# - Standard GPT-2 architecture proven to work, start enabled
# - Reciprocal pathway experimental, start disabled
# - Gradients push reciprocal gates positive only if they reduce loss
# - If reciprocal provides no benefit, stays disabled (zero cost)
#
# Expected training dynamics:
# - Early (0-1000 iters): Trains as standard GPT-2
# - Mid (1000-3000): Gradients learn which layers benefit from reciprocal
# - Late (3000+): Sparse pathway usage emerges per layer
#
# Expected: RA matches or beats baseline by preventing forced use of
# harmful pathways. Reciprocal activates only where beneficial.
#
# Key metrics tracked:
# - skip_std_attn, skip_lowrank_attn: Logits for each pathway
# - use_std_attn, use_lowrank_attn: Binary decisions (sigmoid > 0.5)
# - w_std, w_rec: Pathway weights (geometric init)
# - Perplexity: Final quality vs baseline
#
# Training time: 2 hours (R0 baseline already completed)
#
# Usage:
# make defconfig-gpt2-ra-ablation && make
# Quick test: GPT2_MAX_TIME=600 make (10 minutes)
# Dry-run: make check (~10 seconds)
