# GPT-2 KV Tying Ablation Study on AMD Radeon Pro W7900
# Runs V0 (baseline), V1 (K=V), V2 (K=V.T) sequentially
# Each test runs for 30 minutes (1800s)
# Saves final models only for all 3 variants
# Optimized for AMD Radeon Pro W7900 (48GB VRAM)
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"

# Dataset - Shakespeare (compact and readily available)
# Note: TinyStories requires 'datasets' package: pip install datasets
# Using Shakespeare for immediate testing
CONFIG_GPT2_DATASET_SHAKESPEARE=y
CONFIG_GPT2_DATASET_NAME="shakespeare"

# Model Architecture
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_BIAS=y

# Weight tying (standard)
CONFIG_GPT2_WEIGHT_TYING=y

# Vanilla Ablation Mode - ENABLED
# V0: Baseline GPT-2 (no KV tying)
# V1: GPT-2 with KV tying (K = V)
# V2: GPT-2 with K = V.T (key equals transpose of value)
CONFIG_GPT2_VANILLA_ABLATION_MODE=y
CONFIG_GPT2_VANILLA_ABLATION_STEPS="V0,V1,V2"

# Training Configuration
# Increased gradient accumulation to maintain effective batch size
# effective_batch_size = 20 * 16 = 320 (same as before: 40 * 8)
CONFIG_GPT2_GRADIENT_ACCUMULATION=16
CONFIG_GPT2_MAX_ITERS=10000
CONFIG_GPT2_MAX_TIME=1800
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=200
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"

# Performance Optimizations - W7900 / ROCm optimized
# Disabled torch.compile for ROCm compatibility (potential kernel bugs)
# Disabled flash attention for ROCm compatibility
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=n
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Batch size - reduced for K=V.T (V2) memory requirements
# V2 uses manual attention (materializes full attn matrix)
# V0/V1 use Flash Attention and could handle batch_size=40
CONFIG_BATCH_SIZE=20
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamW
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMW=y
CONFIG_OPTIMIZER="adamw"

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# Experiment Tracking - Unique W&B project for W7900 ablation
CONFIG_ENABLE_TRACKIO=n
CONFIG_ENABLE_WANDB=y
CONFIG_WANDB_PROJECT="gpt2-kv-tying-w7900"
CONFIG_WANDB_ENTITY=""
CONFIG_TRACKER_PROJECT="gpt2-kv-tying-w7900"

# Advanced options
# torch.compile disabled for ROCm stability
CONFIG_COMPILE_MODEL=n
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y

# Checkpointing - ENABLED for model introspection
# Saves final models for V0, V1, V2 for later analysis
# No periodic checkpoints (interval set very high)
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=999999

# Memory Optimizations - ROCm specific
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
# Disable experimental AOTriton for stability
CONFIG_TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=n

# Debugging
CONFIG_DEBUG=n
CONFIG_VERBOSE=y
CONFIG_LOG_LEVEL="INFO"
CONFIG_DRY_RUN=n
