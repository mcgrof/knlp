# knlp: Kernel-Style Machine Learning

**Rapid prototyping and automation for open source ML R&D**

<p align="center">
  <a href="https://xkcd.com/974/">
    <img src="https://imgs.xkcd.com/comics/the_general_problem.png" alt="XKCD: The General Problem" width="400">
  </a>
  <br>
  <em>Our approach to ML automation and the general problem</em>
</p>

Applying Linux kernel development methodologies to machine learning research for rapid iteration and reproducible experimentation. Kconfig-driven configuration, defconfig presets, Makefile automation, and rigorous test matrices enable fast prototyping of transformer architectures, pruning algorithms, and optimization techniques while maintaining reproducibility and collaboration at scale.

<p align="center">
  <a href="https://htmlpreview.github.io/?https://github.com/mcgrof/knlp/blob/main/docs/index.html">
    <strong>Browse Interactive Demos</strong>
  </a>
</p>

### Research Highlights

| Area | Result | Docs | Demo |
|------|--------|------|------|
| **Unified Signal** | [FIM diagonal ≈ Adam exp_avg_sq](https://arxiv.org/abs/2507.18807) — unifies compression, pruning, and tiering | [docs](docs/hierarchical-tiering.md) | [demo](https://htmlpreview.github.io/?https://github.com/mcgrof/knlp/blob/main/docs/fisher_adam_visualization.html) |
| **FIM-Guided Quantization** | Diagonal Fisher identifies critical tensors. **1.26% better PPL** at **1.8% size increase** | [docs](docs/mobile-weight-packing.md) | [demo](https://htmlpreview.github.io/?https://github.com/mcgrof/knlp/blob/main/docs/fim_quantization_visualization.html) |
| **KVSplice** | **~20% extra compression on top of MLA** (7.2x vs 6x), **25% better PPL**, **+7 HellaSwag** | [docs](docs/kvsplice/README.md) | [demo](https://htmlpreview.github.io/?https://github.com/mcgrof/knlp/blob/main/docs/kvsplice_visualization.html) |
| **Reciprocal Attention** | Learned Q@K.T ↔ K@Q.T alternation. **5% better PPL**, **+2 HellaSwag** | [docs](docs/ra.md) | [demo](https://htmlpreview.github.io/?https://github.com/mcgrof/knlp/blob/main/docs/ra_visualization.html) |
| **Adam State-Based Pruning** | bitter7 achieves **15.6% better PPL** than magnitude baseline (37.28 vs 44.15) | [docs](docs/adamwprune_variants.md) | [demo](https://htmlpreview.github.io/?https://github.com/mcgrof/knlp/blob/main/docs/tiering_visualization.html) |
| **Page-Aware GNN Training** | **4× better I/O locality** (6.8× vs 28.5× RA) with **zero quality loss** on DGraphFin | [docs](gnn/docs/gnn-fraud.md) | [demo](https://htmlpreview.github.io/?https://github.com/mcgrof/knlp/blob/main/docs/gnn_fraud_visualization.html) |

## Development Philosophy

knlp applies **Linux kernel development practices** to machine learning research:

- **Kconfig-based configuration**: Hierarchical menus for experiment management (like `make menuconfig`)
- **Defconfig presets**: Reproducible configurations for different hardware and research goals
- **Makefile-driven builds**: Consistent build and test workflows across models
- **Documented decisions**: Every architectural choice explained in `docs/`
- **Rigorous validation**: Automated test matrices before merging experiments

See [docs/architecture.md](docs/architecture.md) for details on the kernel-inspired infrastructure.

## Installation

For systems using `torch.compile()`, Python development headers are required:

```bash
# Ubuntu/Debian
sudo apt-get install python3-dev

# RHEL/CentOS/Fedora
sudo yum install python3-devel
```

```bash
pip install -r requirements.txt
wandb login # optional
```

```bash
make defconfig-gpt2-vanilla-baseline
make
```

See [docs/quickstart.md](docs/quickstart.md) for detailed workflow.

## Contributing

knlp welcomes contributions.

## Citation

If you use this work, please cite:

```bibtex
@misc{knlp2025,
  title        = {knlp: Kernel-Style Machine Learning - Transformer Architecture Research},
  author       = {Luis Chamberlain and contributors},
  year         = {2025},
  howpublished = {\url{https://github.com/mcgrof/knlp}},
  note         = {Collaborative ML research using Linux kernel development workflows}
}
```

## License

This project is licensed under the **MIT License**.

- **Code**: MIT license
- **Models**: AI models generated by this project can be licensed as you choose
- **Documentation**: CC-BY-SA 4.0 (collaborative, share-alike)

See LICENSE for details.
