# SPDX-License-Identifier: MIT
# AdamWPrune Configuration

mainmenu "AdamWPrune v1.0.0 - Neural Network Training Configuration"

menu "General Configuration"

comment "Hyperparameter Auto-Detection - See docs/hyperparameter-auto-detection.md"

choice
	prompt "Hyperparameter configuration mode"
	default HYPER_PARAM_AUTO
	help
	  Choose how batch size and gradient accumulation are determined.

	  AUTO: Automatically detect GPU type, memory, and count to select
	        optimal batch size and gradient accumulation. Recommended
	        for portable configs that work across different hardware.

	  MANUAL: Manually specify batch size and gradient accumulation.
	          Use when you need explicit control over hyperparameters.

config HYPER_PARAM_AUTO
	bool "Auto-detect hyperparameters based on GPU"
	help
	  Automatically detect GPU type (B200, W7900, A10G, etc.), GPU
	  memory, GPU count, and torch.compile() status to select optimal
	  batch size and gradient accumulation steps.

	  The system maintains a constant effective batch size (default 1024)
	  by adjusting batch_size and gradient_accumulation based on:
	  - GPU memory (larger GPU = larger batch)
	  - GPU count (more GPUs = distribute across devices)
	  - torch.compile status (compile = larger batches fit)

	  This eliminates the need for GPU-specific defconfigs and prevents
	  accidentally using wrong hyperparameters for your hardware.

config HYPER_PARAM_MANUAL
	bool "Manually specify hyperparameters"
	help
	  Manually configure batch size and gradient accumulation.
	  Use this when you need explicit control over training hyperparameters.

endchoice

config TARGET_EFFECTIVE_BATCH
	int "Target effective batch size (AUTO mode)"
	default 1024
	range 64 8192
	depends on HYPER_PARAM_AUTO
	help
	  Target effective batch size for AUTO hyperparameter mode.
	  Effective batch = batch_size × gradient_accumulation × num_gpus

	  The system automatically adjusts batch_size and gradient_accumulation
	  to achieve this target while maximizing GPU utilization. The actual
	  batch_size is determined by GPU memory (H100 gets larger batches than
	  A10G), and gradient_accumulation compensates to maintain this target.

	  Model-specific memory scaling factors account for different memory
	  requirements per sample, allowing smaller models to use proportionally
	  larger batches when GPU memory permits:
	    - GPT-2: 1.0x (baseline, large memory per sample)
	    - ResNet-50: 1.5x (medium memory, can use larger batches)
	    - ResNet-18: 2.0x (smaller memory, 2x larger batches)
	    - LeNet-5: 4.0x (tiny memory, 4x larger batches)

	  Common values:
	    - 256: ResNet-50, memory-constrained GPUs
	    - 512: ResNet-18, LeNet-5 (default for CNNs)
	    - 1024: GPT-2 124M (default for transformers)
	    - 2048: Larger models with abundant memory

	  Example: On H100 (80GB) with ResNet-18 and target=512:
	    - batch_size auto-selected as 256 (2x GPT-2's 128)
	    - gradient_accumulation=2 (to reach 512 effective)
	    - Result: 512 effective batch, optimal GPU utilization

config BATCH_SIZE
	int "Batch size for training"
	default 512
	range 1 2048
	depends on HYPER_PARAM_MANUAL
	help
	  Batch size for training. Larger batch sizes can speed up training
	  but require more GPU memory. Default is 512.
	  For GPT-2 with large context (1024), smaller batch sizes (8-16)
	  may be needed to fit in memory.

config NUM_EPOCHS
	int "Number of training epochs"
	default 10
	range 1 1000
	help
	  Number of epochs to train the model. Default is 10.

config LEARNING_RATE
	string "Learning rate"
	default "0.001"
	help
	  Initial learning rate for training. Default is 0.001.

	  IMPORTANT - Model-specific behavior:

	  LeNet-5: Uses this value directly for all optimizers
	  - Typical values: 0.001 for Adam-based, 0.01-0.1 for SGD

	  ResNet-18: Auto-scales for Adam optimizers (resnet18/train.py line ~130)
	  - SGD: Uses the configured value directly
	  - Adam/AdamW/AdamWPrune: Multiplies by 0.001
	  - Example: Set to 0.1 to get SGD=0.1, Adam-based=0.0001

config NUM_WORKERS
	int "Number of data loader workers"
	default 16
	range 0 32
	help
	  Number of parallel workers for data loading. Set to 0 to disable
	  multiprocessing. Default is 16.

config DEVICE
	string "Device for training"
	default "cuda"
	help
	  Device to use for training. Can be "cuda" for GPU or "cpu" for CPU.
	  Default is "cuda" which will automatically fall back to CPU if no
	  GPU is available.

config PYTORCH_CUDA_ALLOC_CONF
	string "PyTorch CUDA memory allocator configuration"
	default "expandable_segments:True"
	help
	  Configuration for PyTorch CUDA memory allocator to prevent OOM.

	  Default: "expandable_segments:True"
	    - Enables expandable memory segments
	    - Prevents memory fragmentation
	    - Critical for avoiding OOM with torch.compile()

	  See /data/TheRock/HIP_OOM_MITIGATION.md for details.

	  This is set as an environment variable BEFORE importing torch.

config TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL
	bool "Enable AOTriton experimental features for ROCm"
	default y
	depends on DEVICE = "cuda"
	help
	  Enable experimental AOTriton features for ROCm/HIP backend.
	  Required for flash attention support on AMD GPUs.

	  When enabled, sets TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1
	  before importing torch.

	  This enables memory efficient attention (flash attention) which
	  provides 2-4x memory reduction for attention layers.

config DATA_DIR
	string "Data directory path"
	default "data"
	help
	  Path to the directory where datasets will be stored/loaded.
	  Default is "data" in the project root.

config OUTPUT_DIR
	string "Output directory for results"
	default "results"
	help
	  Directory where training results, models, and metrics will be saved.
	  Default is "results".

config JSON_OUTPUT
	string "JSON metrics output filename"
	default "training_metrics.json"
	help
	  Filename for saving training metrics in JSON format.
	  Default is "training_metrics.json".

endmenu

source "Kconfig.models"
source "Kconfig.optimizers"
source "Kconfig.pruning"
source "Kconfig.ra_mla"

menu "Advanced Options"

choice
	prompt "Model compilation mode"
	default COMPILE_AUTO
	depends on DEVICE = "cuda"
	help
	  Choose how torch.compile() is enabled/disabled.

	  AUTO: Automatically enable/disable based on GPU type and driver quality
	  MANUAL: Manually control torch.compile() setting

config COMPILE_AUTO
	bool "Auto-detect compilation based on GPU"
	help
	  Automatically enable or disable torch.compile() based on GPU type:
	  - NVIDIA GPUs: ENABLED (good torch.compile support)
	  - AMD W7900: DISABLED (ROCm torch.compile issues)
	  - Other AMD: ENABLED by default

	  This prevents issues with GPU/driver combinations where torch.compile
	  causes crashes, OOMs, or performance degradation.

config COMPILE_MANUAL
	bool "Manually control compilation"
	help
	  Manually specify whether to use torch.compile().
	  Use this when you want explicit control.

endchoice

config COMPILE_MODEL
	bool "Enable torch.compile() (MANUAL mode)"
	default y
	depends on COMPILE_MANUAL
	help
	  Enable PyTorch 2.0+ torch.compile() for faster execution.
	  Only used when COMPILE_MANUAL is selected.

config MIXED_PRECISION
	bool "Enable mixed precision training (AMP)"
	default y
	depends on DEVICE = "cuda"
	help
	  Enable Automatic Mixed Precision (AMP) training for faster
	  training and reduced memory usage on GPUs.

config GPU_WARMUP
	bool "Enable GPU warmup"
	default y
	depends on DEVICE = "cuda"
	help
	  Perform GPU warmup iterations before training to ensure
	  consistent timing measurements.

config PIN_MEMORY
	bool "Pin memory for data loading"
	default y
	depends on DEVICE = "cuda"
	help
	  Pin memory for faster GPU transfer during data loading.

config GPU_MONITOR
	bool "Enable GPU memory monitoring"
	default n
	depends on DEVICE = "cuda"
	help
	  Monitor GPU memory usage during training and inference.
	  Saves data to JSON files for analysis and visualization.

config PERSISTENT_WORKERS
	bool "Keep data loader workers persistent"
	default y
	depends on NUM_WORKERS > 0
	help
	  Keep data loader workers alive between epochs to reduce overhead.

config PREFETCH_FACTOR
	int "Prefetch factor for data loading"
	default 2
	range 1 10
	depends on NUM_WORKERS > 0
	help
	  Number of batches to prefetch per worker. Default is 2.


config SAVE_CHECKPOINT
	bool "Save model checkpoints"
	default y
	help
	  Save model checkpoints during training.

config CHECKPOINT_INTERVAL
	int "Checkpoint save interval (iterations)"
	default 1000
	range 1 9999999
	depends on SAVE_CHECKPOINT
	help
	  Save checkpoints every N epochs. Default is 1.

endmenu

menu "Experiment Tracking"

comment "Enable any combination of trackers (they can run simultaneously)"
comment "CLI: make defconfig TRACKER=wandb,trackio (or both, none, wandb, trackio)"

# CLI override detection
config TRACKER_SET_BY_CLI
	bool
	default $(shell, scripts/check-cli-set-var.sh TRACKER)

config TRACKER_CLI_VALUE
	string
	default "$(TRACKER)" if TRACKER_SET_BY_CLI
	default ""

config ENABLE_TRACKIO
	bool "Enable Trackio (local tracking)"
	default y if TRACKER_SET_BY_CLI && $(shell, scripts/check-tracker-enabled.sh trackio $(TRACKER)) = "y"
	default n
	help
	  Use Trackio for lightweight, local-first experiment tracking.
	  Stores data in local SQLite database with web dashboard.
	  Install with: pip install trackio
	  Can be used together with WandB for comparison.
	  CLI: make defconfig TRACKER=trackio
	       make defconfig TRACKER=wandb,trackio

config ENABLE_WANDB
	bool "Enable Weights & Biases (cloud tracking)"
	default y if TRACKER_SET_BY_CLI && $(shell, scripts/check-tracker-enabled.sh wandb $(TRACKER)) = "y"
	default n
	help
	  Use Weights & Biases for cloud-based experiment tracking.
	  Provides advanced features like sweeps and team collaboration.
	  Install with: pip install wandb
	  Can be used together with Trackio for comparison.
	  CLI: make defconfig TRACKER=wandb
	       make defconfig TRACKER=wandb,trackio

config TRACKER_PROJECT
	string "Project name for experiment tracking"
	default "$(shell, scripts/generate-project-name.sh $(MODEL) tracking)"
	depends on ENABLE_TRACKIO || ENABLE_WANDB
	help
	  Project name to organize experiments in the tracking system.
	  Auto-generated as: {model}-{checksum}
	  where model is the selected model and checksum is derived from
	  hostname, IP, and current directory for uniqueness.
	  Used by both Trackio and WandB when enabled.

config TRACKER_RUN_NAME
	string "Run name for experiment tracking"
	default ""
	depends on ENABLE_TRACKIO || ENABLE_WANDB
	help
	  Optional custom name for this training run.
	  If empty, will be auto-generated with format:
	  {model}_{optimizer}_{timestamp}

config WANDB_PROJECT
	string "WandB-specific project name (overrides TRACKER_PROJECT)"
	default ""
	depends on ENABLE_WANDB
	help
	  Optional WandB-specific project name.
	  If empty, uses TRACKER_PROJECT.
	  Will be used for wandb.init(project=...).

config WANDB_ENTITY
	string "WandB entity (team/username)"
	default ""
	depends on ENABLE_WANDB
	help
	  WandB entity (team or username) for experiment tracking.
	  Leave empty to use default entity.

config WANDB_OFFLINE
	bool "Run WandB in offline mode"
	default n
	depends on ENABLE_WANDB
	help
	  Run WandB in offline mode without requiring login.
	  Runs can be synced later with 'wandb sync'.

config BASELINE_RUN_ID
	string "Baseline run ID for comparison (entity/project/run_id)"
	default ""
	depends on ENABLE_WANDB || ENABLE_TRACKIO
	help
	  Reference a previous baseline run instead of re-running baseline step.

	  Format: entity/project/run_id (for W&B)

	  When set, ablation studies skip baseline steps (V0, M0, L0, S0, R0, C0)
	  and copy the specified run to the current project for comparison.

	  Workflow:
	    make defconfig-gpt2-r-mlp-prune BASELINE=mcgrof/old-project/abc123
	    make BASELINE=mcgrof/old-project/abc123

	  Note: BASELINE must be specified on both commands to ensure config.py
	  is regenerated with the baseline reference before execution.

	  This automatically:
	    1. Copies the baseline run to current project
	    2. Filters baseline step from test matrix
	    3. Runs only non-baseline steps (M1, M2, M3, etc.)

	  Useful for comparing experiments without re-running expensive baselines.

config TRACKIO_PORT
	int "Port for Trackio dashboard"
	default 7860
	depends on ENABLE_TRACKIO
	help
	  Port number for the Trackio web dashboard.
	  Access dashboard with: trackio show --port PORT

endmenu

menu "Reciprocal Attention (RA) + MLA (Experimental)"

config ENABLE_RA_MLA
	bool "Enable RA+MLA experimental features"
	default n
	help
	  Enable Reciprocal Attention and Multi-Latent Attention
	  experimental features for GPT-2 training.

config RA_MLA_ABLATION_MODE
	bool "Enable RA+MLA ablation study mode"
	default n
	help
	  Run ablation study testing different RA/MLA configurations.
	  Disables standard test matrix mode.

config RA_MLA_ABLATION_STEPS
	string "Ablation steps to run (comma-separated)"
	default "V0,V1,V2,V3"
	depends on RA_MLA_ABLATION_MODE
	help
	  Comma-separated list of ablation steps to run.
	  Example: "V0,V1,V2,V3" or "V0,V11,V12,V13,V14,V15"

config TEST_MATRIX_MODE
	bool "Enable standard test matrix mode"
	default y
	depends on !RA_MLA_ABLATION_MODE
	help
	  Run standard test matrix testing optimizer/pruning combinations.
	  Cannot be used simultaneously with ablation mode.

endmenu

menu "Hierarchical Memory Tiering (Experimental)"

config ENABLE_HIERARCHICAL_TIERING
	bool "Enable hierarchical memory tiering"
	default n
	help
	  Enable hierarchical memory tiering to place model weights across
	  different memory tiers (GPU HBM, CPU RAM, SSD) based on usage
	  patterns inferred from Adam optimizer states.

	  This can reduce memory pressure on GPU while maintaining good
	  performance for frequently accessed weights.

config TIERING_METHOD
	string
	default "none" if !ENABLE_HIERARCHICAL_TIERING
	default "adam_state" if TIERING_ADAM_STATE

choice
	prompt "Tiering strategy"
	default TIERING_ADAM_STATE
	depends on ENABLE_HIERARCHICAL_TIERING
	help
	  Choose which strategy to use for determining weight placement
	  across memory tiers.

config TIERING_ADAM_STATE
	bool "Adam state-based tiering"
	help
	  Analyze Adam optimizer states (momentum, variance) to infer
	  which weights are frequently updated vs stable. Frequently
	  updated weights stay in HBM, stable weights move to slower
	  tiers (CPU RAM, SSD).

endchoice

choice
	prompt "Tiering mode"
	default TIERING_EMULATED
	depends on ENABLE_HIERARCHICAL_TIERING
	help
	  Choose between emulated delays (for testing without real
	  hardware changes) or real offloading (actual movement).

config TIERING_EMULATED
	bool "Emulated tiering (fake delays)"
	help
	  Use forward hooks to inject realistic latency delays based on
	  tier placement, but keep all weights on GPU. This allows testing
	  tier strategies without actual memory movement or specialized
	  hardware (CXL, fabric-attached memory, etc).

	  Latency model accounts for:
	  - Setup overhead (PCIe, fabric latency)
	  - Transfer bandwidth (HBM: ~800 GB/s, DDR5: ~150 GB/s, NVMe: ~10 GB/s)

config TIERING_REAL_OFFLOAD
	bool "Real offloading (CPU/disk)"
	help
	  Actually move weights to CPU RAM or disk storage using PyTorch
	  hooks and device_map. Weights are loaded to GPU just before use
	  and evicted afterward.

	  This reduces GPU memory usage but may impact performance if tier
	  assignments are too aggressive. Requires careful tuning.

endchoice

config TIERING_GENERATE_JSON
	bool "Generate tier hints JSON after training"
	default y
	depends on ENABLE_HIERARCHICAL_TIERING && TIERING_ADAM_STATE
	help
	  After training completes, analyze Adam optimizer states and
	  generate a JSON file with tier placement hints for each module.
	  Format: {"transformer.h.0.attn": "HBM", "transformer.h.5.mlp": "CPU", ...}

	  This JSON can be used for inference benchmarking to evaluate
	  impact of tier placement on inference latency.

config TIERING_JSON_OUTPUT
	string "Tier hints JSON output path"
	default "tier_hints.json"
	depends on TIERING_GENERATE_JSON
	help
	  Path where tier placement hints JSON will be written.

config TIERING_HBM_THRESHOLD
	string "HBM tier threshold (top percentile)"
	default "0.3"
	depends on ENABLE_HIERARCHICAL_TIERING
	help
	  Fraction of weights to keep in HBM (GPU memory). Most frequently
	  accessed weights stay here. Default: 30% in HBM.

config TIERING_CPU_THRESHOLD
	string "CPU tier threshold (middle percentile)"
	default "0.5"
	depends on ENABLE_HIERARCHICAL_TIERING
	help
	  Fraction of weights to place in CPU RAM (offloaded from GPU but
	  fast to access). Moderately accessed weights. Default: 50% in CPU.

	  Remaining weights go to SSD (slowest tier).

config TIERING_INFERENCE_BENCHMARK
	bool "Run inference benchmark after training"
	default n
	depends on TIERING_GENERATE_JSON
	help
	  After training and generating tier hints, run inference benchmark
	  to measure impact of tier placement on inference latency and
	  throughput.

endmenu

menu "Test Matrix"

config TEST_RESULTS_DIR
	string "Test results directory for graph generation"
	default ""
	help
	  Specify the test matrix results directory to use for generating graphs.
	  Leave empty to use the most recent test_matrix_results_* directory.
	  Example: test_matrix_results_20250826_181029

config AUTO_GENERATE_GRAPHS
	bool "Automatically generate graphs after test matrix"
	default y
	help
	  Automatically generate comparison graphs for each optimizer after
	  completing the test matrix run.

config PARALLEL_JOBS
	int "Number of parallel training jobs"
	default 1
	range 1 50
	help
	  Number of training jobs to run in parallel. Each job uses minimal
	  GPU memory (~50-100MB for LeNet-5), so multiple jobs can run
	  simultaneously on high-memory GPUs. Recommended: 1-4 for <16GB GPU,
	  8-20 for 24-48GB GPU, up to 50 for very large GPUs.

config MAX_GPU_MEMORY_PERCENT
	int "Maximum GPU memory usage percentage"
	default 90
	range 50 95
	depends on PARALLEL_JOBS > 1
	help
	  Maximum percentage of GPU memory to use for parallel training.
	  The system will automatically limit concurrent jobs to stay within
	  this threshold. Default is 90%.

config PARALLEL_BATCH_SIZE
	int "Batch size for parallel jobs"
	default 256
	range 32 1024
	depends on PARALLEL_JOBS > 1
	help
	  Batch size to use when running parallel jobs. Smaller batch sizes
	  reduce memory usage per job, allowing more parallel execution.
	  Default is 256 (half of normal batch size).

endmenu

menu "Debugging"

config DEBUG
	bool "Enable debug mode"
	default n
	help
	  Enable debug mode with verbose logging and additional checks.

config VERBOSE
	bool "Enable verbose output"
	default y
	help
	  Enable verbose output during training.

config LOG_LEVEL
	string "Logging level"
	default "INFO"
	help
	  Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).
	  Default is INFO.

config PROFILE
	bool "Enable profiling"
	default n
	help
	  Enable performance profiling during training.

config DRY_RUN
	bool "Dry-run mode: architecture validation only"
	default n
	help
	  Enable dry-run mode for quick architecture validation without GPU.
	  Creates model, runs single forward/backward/optimizer step with
	  dummy data on CPU, then exits. Catches configuration and
	  architecture errors in seconds instead of hours.

	  Use 'make check' to validate all ablation steps automatically.

config INFERENCE_TEST
	bool "Test inference after training"
	default n
	help
	  Run inference testing and memory measurement after training completes.
	  This will measure actual GPU memory usage during inference with various
	  batch sizes and compare memory usage across different models.

config INFERENCE_BATCH_SIZES
	string "Batch sizes for inference testing"
	default "1,32,128,256"
	depends on INFERENCE_TEST
	help
	  Comma-separated list of batch sizes to test during inference.
	  Default is "1,32,128,256".

config SAVE_CHECKPOINT
	bool "Save model checkpoint after training"
	default y
	help
	  Save the trained model checkpoint after training completes.
	  Required for inference testing. Models can be large (180MB+ for ResNet-18).

endmenu
