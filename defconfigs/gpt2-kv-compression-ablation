# GPT-2 KV Cache Compression Ablation Study
# Tests V-only pruning + Spline->PCA geometric compression (KV-Geometry)
#
# Hardware: 4× NVIDIA A10G (24GB each) with DDP
# Research: How does geometric compression interact with V-only pruning?
#
# To run: make defconfig-gpt2-kv-compression-ablation && make
#
# Training modes:
# 1. Time-based (recommended): Set CONFIG_GPT2_MAX_TIME for fixed duration
#    - 2 hours:  CONFIG_GPT2_MAX_TIME=7200
#    - 8 hours:  CONFIG_GPT2_MAX_TIME=28800
#    - Override: GPT2_MAX_TIME=7200 make
# 2. Iteration-based: Set CONFIG_GPT2_MAX_ITERS (traditional)

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
# Time-based training (2 hours per step for quality convergence)
# For quick test, use: GPT2_MAX_TIME=600 make (10 minutes)
CONFIG_GPT2_MAX_TIME=7200
# Iteration-based alternative (uncomment to use instead of time-based)
# CONFIG_GPT2_MAX_ITERS=10400
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=200
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=y
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters - GPU-optimized for A10G 24GB
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration - Disabled (using ablation mode instead)
CONFIG_ENABLE_RA_MLA=n

# Enable ablation study mode with V-pruning + KV-Geometry compression
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="V0,V19,C1,C2,C3"

# Disable test matrix mode - ablation mode runs its own test sequence
CONFIG_TEST_MATRIX_MODE=n

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=n
CONFIG_TRACKER_PROJECT="gpt2-kv-compression"

# Advanced options
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Test execution
CONFIG_TEST_PARALLEL_JOBS=1
CONFIG_OUTPUT_DIR="test_matrix_results_kv_compression"

# Ablation step details:
#
# BASELINE - Step V0
# ==================
# - Step V0: Pure baseline GPT-2
#   Architecture: Standard SDPA attention, standard MLP
#   Purpose: Reference point for all memory/quality comparisons
#
# V-ONLY PRUNING - Step V19
# ==========================
# - Step V19: V-only pruning (clean isolation test)
#   Architecture: Keep K full, prune V to 391 tokens (38.2%)
#   KV pruning: Fixed golden ratio, exposure correction enabled
#   Memory: 391 × 64 = 25,024 per head (62% reduction vs baseline)
#   Purpose: Isolate V-only pruning effect without other mechanisms
#
# KV-GEOMETRY COMPRESSION - Steps C1-C3
# ======================================
# All C-steps use Spline->PCA geometric compression on V vectors.
# Compression learned via calibration: collect V samples, fit geometry,
# save to kvgeom.pt. Each step tests different compression ratios.
#
# - Step C1: V-only pruning + light compression (k=32)
#   V compression: 64 → 32 dims (50% reduction per vector)
#   Total memory: 391 × 32 = 12,512 per head (81% reduction vs baseline)
#   Purpose: Conservative compression - does geometry help at 2:1 ratio?
#
# - Step C2: V-only pruning + medium compression (k=16)
#   V compression: 64 → 16 dims (75% reduction per vector)
#   Total memory: 391 × 16 = 6,256 per head (90% reduction vs baseline)
#   Purpose: Aggressive compression - quality vs memory tradeoff
#
# - Step C3: V-only pruning + heavy compression (k=8)
#   V compression: 64 → 8 dims (87.5% reduction per vector)
#   Total memory: 391 × 8 = 3,128 per head (95% reduction vs baseline)
#   Purpose: Extreme compression - lower bound for usable quality
#
# Memory comparison table:
# ========================
# Step  | V cache size      | Reduction | Notes
# ------|-------------------|-----------|----------------------------------
# V0    | 1024 × 64 = 65536 | 0%        | Baseline reference
# V19   | 391 × 64 = 25024  | 62%       | V-only pruning alone
# C1    | 391 × 32 = 12512  | 81%       | Pruning + light compression
# C2    | 391 × 16 = 6256   | 90%       | Pruning + medium compression
# C3    | 391 × 8 = 3128    | 95%       | Pruning + heavy compression
#
# Expected comparisons:
# - V19 vs V0: V-only pruning baseline (should match or improve V0)
# - C1 vs V19: Does 2:1 geometric compression hurt quality?
# - C2 vs C1: Is 4:1 compression viable with Spline->PCA geometry?
# - C3 vs C2: Where is the quality cliff for geometric compression?
#
# KV-Geometry calibration:
# - Enabled for C1-C3 steps via --kvgeom-enable
# - Collects 120k V samples during warmup (64 batches)
# - Fits Spline->PCA with 7 knots, 8 epochs, lr=2e-3
# - Saves geometry to kvgeom_{step}.pt for inference
#
# Usage:
# 1. Full ablation (2 hours/step, 10 hours total for 5 steps):
#    make defconfig-gpt2-kv-compression-ablation && make
#
# 2. Quick sanity check (60 seconds/step):
#    make defconfig-gpt2-kv-compression-ablation
#    GPT2_MAX_TIME=60 make
#
# 3. Dry-run validation (CPU, ~25 seconds total):
#    make defconfig-gpt2-kv-compression-ablation
#    make check
#
