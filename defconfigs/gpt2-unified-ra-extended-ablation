# Unified RA: Extended 7-Step Ablation Study
# Tests Unified RA with different configurations (V0-V6)
#
# Hardware: 4× NVIDIA A10G (24GB each) with DDP
# Research question: How do different R values, self-restart, and MLP sizes affect quality?
#
# To run: make defconfig-gpt2-unified-ra-extended-ablation && make
#
# Training modes:
# 1. Time-based (recommended): Set CONFIG_GPT2_MAX_TIME for fixed duration
#    - 2 hours:  CONFIG_GPT2_MAX_TIME=7200
#    - 8 hours:  CONFIG_GPT2_MAX_TIME=28800
#    - Override: GPT2_MAX_TIME=7200 make
# 2. Iteration-based: Set CONFIG_GPT2_MAX_ITERS (traditional)

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
# Time-based training (2 hours per step for quality comparison)
CONFIG_GPT2_MAX_TIME=7200
# Iteration-based alternative (uncomment to use instead of time-based)
# CONFIG_GPT2_MAX_ITERS=10400
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=y
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters - GPU-optimized for A10G 24GB
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=128
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.3"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
CONFIG_RA_MLA_USE_FLASH=n
CONFIG_RA_MLA_LOG_METRICS=n
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
CONFIG_RA_MLA_USE_ROPE=n

# R-MLP Configuration (controlled per-step in ablation)
CONFIG_RA_MLA_MLP_ATTN_GATE=n
CONFIG_RA_MLA_MLP_CROSS_TOKEN=n
CONFIG_RA_MLA_MLP_LATENT_RECIP=n

# Enable ablation study mode with Unified RA + R-MLP steps
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="V0,V1,V2,V3,V4,V5,V6"

# Disable test matrix mode - ablation mode runs its own test sequence
CONFIG_TEST_MATRIX_MODE=n

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=n
CONFIG_TRACKER_PROJECT="gpt2-unified-ra-rmlp-ablation"

# Advanced options
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Test execution
CONFIG_TEST_PARALLEL_JOBS=1
CONFIG_OUTPUT_DIR="test_matrix_results_unified_ra_rmlp"

# Ablation step details:
#
# RECIPROCAL ATTENTION (RA) - Steps V0-V2
# ========================================
# - Step V0: Baseline GPT-2 (standard SDPA)
#   Speed: 1.33ms (eager), ~1.15ms (compiled - expect 13.5% speedup)
#   Architecture: Standard Q,K,V projection + SDPA
#   Purpose: Control baseline for RA comparison
#
# - Step V1: Unified RA (R=4, direct folded layout)
#   Speed: 1.33ms eager (MATCHES baseline!), ~1.15ms compiled
#   Architecture: Folded [Qf|Kf] emission, per-head learned gates (w_std, w_rec)
#   Key principle: Split per-head dimension D into (D_std + R) and use fused
#   projection to emit [Q_std|K_low] and [K_std|Q_low], so reciprocal attention
#   is computed inside same SDPA call without increasing FLOPs.
#   Purpose: Test RA reciprocity at zero overhead
#
# - Step V2: Unified RA + Self-Restart
#   Speed: ~1.33ms (minimal overhead)
#   Architecture: V1 + identity path: out = (1-α)·attn + α·V
#   Per-head learnable α (init 0.05, clamped [0, 0.5])
#   Purpose: Test if identity residual improves training stability
#
# RECIPROCAL MLP (R-MLP) - Steps V3-V6
# ======================================
# All R-MLP steps build on Unified RA (V1) as attention foundation
#
# - Step V3: Unified RA + R-MLP (MLP_ATTN_GATE)
#   Architecture: V1 + Mechanism 1 (MLP-to-Attention gating)
#   Mechanism: MLP activations modulate attention head weights in next layer
#   Parameters: gate_dim=64, gate_alpha=0.1
#   Purpose: Test MLP→Attention information flow
#
# - Step V4: Unified RA + R-MLP (MLP_CROSS_TOKEN)
#   Architecture: V1 + Mechanism 2 (Cross-token MLP aggregation)
#   Mechanism: MLP receives weighted sum from other tokens using attention weights
#   Parameters: cross_alpha=0.3, topk=8 sparsification
#   Purpose: Test attention-guided MLP aggregation
#
# - Step V5: Unified RA + R-MLP (MLP_LATENT_RECIP)
#   Architecture: V1 + Mechanism 3 (MLP latent space reciprocity)
#   Mechanism: Bidirectional pathways between attention/MLP latents
#   Parameters: latent_dim=128, recip_alpha=0.2, tied_transpose
#   Purpose: Test latent-space coupling between attention and MLP
#
# - Step V6: Unified RA + R-MLP (ALL mechanisms)
#   Architecture: V1 + Mechanisms 1+2+3 enabled together
#   Purpose: Test composition of all R-MLP mechanisms
#
# Research Questions:
# 1. Does Unified RA (V1) match/exceed baseline (V0) at same speed?
# 2. Does Self-Restart (V2) improve stability over V1?
# 3. Which R-MLP mechanism (V3-V5) provides best quality gains?
# 4. Do R-MLP mechanisms compose well (V6 vs individual V3-V5)?
#
# Usage examples:
# 1. Default time-based training (2 hours per step - recommended):
#    make defconfig-gpt2-unified-ra-rmlp-ablation && make
#
# 2. Extended training (8 hours per step for thorough validation):
#    make defconfig-gpt2-unified-ra-rmlp-ablation
#    GPT2_MAX_TIME=28800 make
#
# 3. Quick test (30 minutes per step for debugging):
#    make defconfig-gpt2-unified-ra-rmlp-ablation
#    GPT2_MAX_TIME=1800 make
#
# 4. Dry-run validation (test all steps quickly):
#    make defconfig-gpt2-unified-ra-rmlp-ablation
#    make check
#
# Expected total time: 7 steps × 2 hours = 14 hours (time-based default)
# Expected total time: 7 steps × 8 hours = 56 hours (extended validation)
