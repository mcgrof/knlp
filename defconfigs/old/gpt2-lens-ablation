#
# GPT-2 Lens-Gated Route Gate Ablation (L5-L7)
# Re-run tests that failed with route_gate=0.000 bug
#
# L5: Full lens + route gate (learned attention/MLP ratio)
# L6: L5 + MLP context + K/V compression
# L7: L6 + conductor mode (conditional MLP context)
#
# Memory-optimized for A10G (24GB):
#   - Reduced batch_size=6 (down from 8)
#   - Gradient accumulation=8 (effective batch=48)
#
# Time-based testing:
#   - Target: 786 minutes (same as L0 baseline wall time)
#   - Expected: ~7000ms/iter → 6,700 iterations
#   - Fair A/B test: same GPU hours, different convergence
#

# Model Selection - GPT-2
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL="gpt2"
CONFIG_TEST_MODELS="gpt2"
CONFIG_GPT2_MODEL_SIZE="124M"

# Disable compilation to save memory
# CONFIG_COMPILE_MODEL is not set

# Enable test matrix mode for L5-L7 ablation
CONFIG_TEST_MATRIX_MODE=y

# Optimizer Configuration - AdamWSPAM only
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER_ENABLED_ADAMWSPAM=y
CONFIG_OPTIMIZER_DEFAULT="adamwspam"

# SPAM settings
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_PERIODIC_RESET=y

# Pruning Configuration - None (focus on architecture)
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_ENABLED_NONE=y
CONFIG_PRUNING_DEFAULT="none"

# Single-GPU Configuration (A10G 24GB)
# CONFIG_GPT2_USE_DDP is not set
CONFIG_GPT2_DDP_NUM_GPUS=1

# Training Parameters - Memory-optimized for A10G
# Reduced batch size to avoid OOM in L6
CONFIG_BATCH_SIZE=6
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
CONFIG_NUM_EPOCHS=10

# Time-based iteration count (same wall-clock as baseline)
# Baseline: 4535ms/iter × 10,400 = 786 minutes
# Route gate tests: ~7000ms/iter × 6,700 ≈ 786 minutes
CONFIG_GPT2_MAX_ITERS=6700
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10

# Learning Rate Schedule
CONFIG_LEARNING_RATE="6e-4"
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y

# Dataset Configuration
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024

# Data Loading Optimization
CONFIG_NUM_WORKERS=16
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Mixed Precision Training
CONFIG_MIXED_PRECISION=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Performance Optimizations
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPU_WARMUP=y

# Memory Optimizations for 24GB GPU
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Checkpointing
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000
CONFIG_OUTPUT_DIR="./results/gpt2-lens-route-gate"

# Experiment Tracking - W&B and Trackio
CONFIG_GPT2_TRACKER="wandb,trackio"
CONFIG_TRACKER_PROJECT="gpt2-lens-ablation"

# Reciprocal Attention (RA) + MLA Configuration
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=64
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.5"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
# CONFIG_RA_MLA_USE_FLASH is not set
CONFIG_RA_MLA_LOG_METRICS=y
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
# CONFIG_RA_MLA_USE_ROPE is not set

# Disable reciprocal MLP mechanisms (not used in L5-L7)
# CONFIG_RA_MLA_MLP_ATTN_GATE is not set
# CONFIG_RA_MLA_MLP_CROSS_TOKEN is not set
# CONFIG_RA_MLA_MLP_LATENT_RECIP is not set
# CONFIG_RA_MLA_BITTER_SCALE is not set

# Lens-Gated Ablation Study - Enable ONLY L5, L6, L7
CONFIG_RA_MLA_ABLATION_MODE=y
# CONFIG_RA_MLA_ABLATION_BASELINE is not set
# CONFIG_RA_MLA_ABLATION_STEP1 is not set
# CONFIG_RA_MLA_ABLATION_STEP2 is not set
# CONFIG_RA_MLA_ABLATION_STEP3 is not set
# CONFIG_RA_MLA_ABLATION_STEP4 is not set
CONFIG_RA_MLA_ABLATION_STEP5=y
CONFIG_RA_MLA_ABLATION_STEP6=y
CONFIG_RA_MLA_ABLATION_STEP7=y
# CONFIG_RA_MLA_ABLATION_STEP8 is not set
# CONFIG_RA_MLA_ABLATION_STEP9 is not set
# CONFIG_RA_MLA_ABLATION_STEP10 is not set
# CONFIG_RA_MLA_ABLATION_STEP11 is not set
# CONFIG_RA_MLA_ABLATION_STEP12 is not set
# CONFIG_RA_MLA_ABLATION_STEP13 is not set
# CONFIG_RA_MLA_ABLATION_STEP14 is not set
CONFIG_RA_MLA_ABLATION_STEPS="L5,L6,L7"

# Advanced Options
# CONFIG_GPU_MONITOR is not set
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=100

# Test Matrix
CONFIG_TEST_RESULTS_DIR=""
CONFIG_AUTO_GENERATE_GRAPHS=y
CONFIG_PARALLEL_JOBS=1

# Debugging
# CONFIG_DEBUG is not set
CONFIG_VERBOSE=y
CONFIG_LOG_LEVEL="INFO"
# CONFIG_PROFILE is not set
# CONFIG_DRY_RUN is not set
# CONFIG_INFERENCE_TEST is not set
