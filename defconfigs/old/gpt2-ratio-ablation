# RATIO Framework: 15-Step Ablation Study Configuration (4× A10G GPUs)
# Tests golden ratio (1:2.5), MLP mechanisms, RA, MLA, and structure-aware optimization
#
# Hardware: 4× NVIDIA A10G (24GB each) with DDP
# Based on: gpt2-ra-mla-full proven settings for A10G×4
# Victory condition: Step 14 (full RATIO) > Step 1 (SPAM pruning baseline)
#
# To run: make defconfig-gpt2-ratio-ablation && make

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration - GPU-aligned for tensor core optimization
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
CONFIG_GPT2_MAX_ITERS=10400
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=n
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters - GPU-optimized for A10G 24GB with tensor core alignment
# Original baseline: batch_size=16 × gradient_accumulation=4=effective_batch_size=64
# Fixed config: batch_size=8 × gradient_accumulation=8=effective_batch_size=64 (SAME!)
# Maintains identical optimization dynamics while preventing OOM
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM (for all steps except step 1 which uses pruning)
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled (will be enabled per-step for steps 1 and 14)
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration for RATIO ablation
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=128
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.3"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
CONFIG_RA_MLA_USE_FLASH=n
# CRITICAL FIX: Disable metrics logging to prevent OOM during entropy computation
# Original test_matrix_results_20251102_032010 failed steps 2+ with OOM (768MB alloc)
CONFIG_RA_MLA_LOG_METRICS=n
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
CONFIG_RA_MLA_USE_ROPE=n

# Enable ablation study mode with all 19 steps (including RA-CT)
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18"

# Disable test matrix mode - ablation mode runs its own test sequence
CONFIG_TEST_MATRIX_MODE=n

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=n
CONFIG_TRACKER_PROJECT="gpt2-ratio-ablation"

# Advanced options - torch.compile disabled for RA+MLA (inductor bug)
CONFIG_COMPILE_MODEL=n
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Memory Optimizations - CRITICAL for A10G GPUs preventing fragmentation
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Test execution
CONFIG_TEST_PARALLEL_JOBS=1
CONFIG_OUTPUT_DIR="test_matrix_results_ratio_ablation"

# GPU-Aligned Dimensions (all multiples of 64 for tensor core optimization):
# Following Karpathy's nanoGPT principle: vocab 50257→50304 for optimized kernels
# - d_model=768=12×64 ✓
# - head_dim=64=1×64 ✓ (optimal for tensor cores)
# - latent_dim=128=2×64 ✓
# - mlp_dim (step 2,6): 3840=60×64 ✓
# - mlp_dim (step 3): 3264=51×64 ✓
# - mlp_dim (step 4,5,7,8): 3072=48×64 ✓
# - mlp_dim (step 9,11): 2560=40×64 ✓ (expansion_ratio=3.3333333333)
# - mlp_dim (step 10,12,13,14): 2048=32×64 ✓ (expansion_ratio=2.6666666667)
#
# Fixes applied in this version:
# 1. CONFIG_BATCH_SIZE=8 + CONFIG_GPT2_GRADIENT_ACCUMULATION=8 (maintains effective_batch=64)
# 2. CONFIG_RA_MLA_LOG_METRICS=n (disables entropy computation to prevent OOM)
# 3. train_ra_mla.py handles MLP dimension mismatch when copying pretrained weights
# 4. train_ra_mla.py uses precise expansion ratios for GPU-aligned dimensions
# 5. ra_mla_gpt2.py has memory-efficient entropy computation (when enabled)
#
# Ablation step details (see docs/ra.md for full description):
# - Step 0: Baseline GPT-2 (ratio 1:2.0, mlp_dim=3072)
# - Step 1: Baseline + SPAM pruning 50%
# - Step 2: Golden ratio 1:2.5 (mlp_dim=3840)
# - Step 3: Step 2 + MLP gating 15% (mlp_dim=3264)
# - Step 4: Step 3 + cross-token 10% (mlp_dim=3072)
# - Step 5: Baseline + RA (ra_alpha=0.3)
# - Step 6: RA + golden ratio (mlp_dim=3840)
# - Step 7: Step 6 + mechanisms (mlp_dim=3072)
# - Step 8: Baseline + MLA (latent_dim=128, ratio 1:3.0)
# - Step 9: MLA + golden ratio (mlp_dim=2560, ratio 1:2.5)
# - Step 10: Step 9 + mechanisms (mlp_dim=2048)
# - Step 11: MLA + RA + golden ratio (mlp_dim=2560)
# - Step 12: Step 11 + mechanisms (mlp_dim=2048)
# - Step 13: Baseline + RA-CT (topk, output mode, alpha=0.2)
# - Step 14: MLA + RA-CT (compression + cross-token gating)
# - Step 15: MLA + RA + RA-CT (full attention stack)
# - Step 16: Full RATIO (step 12 + RA-CT, all mechanisms)
# - Step 17: RA-CT weights mode (vs step 13 output mode)
# - Step 18: RA-CT entropy mode (adaptive gating)
