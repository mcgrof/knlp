# RATIO Framework: Reciprocal MLP Mechanisms (6 missing steps)
# Tests MLP-to-Attention gating and cross-token aggregation mechanisms
#
# This defconfig runs only the steps that failed in the initial
# ratio ablation run due to outdated code on the remote machine.
# All these steps test reciprocal MLP mechanisms (bidirectional
# MLP↔Attention information flow).
#
# Failed steps: 3, 4, 7, 10, 12, 16
# Reason: Overly strict assertions removed in commit 3c40bb2
# Run after: git pull on remote machine to sync latest code
#
# To run: make defconfig-gpt2-ratio-ablation-reciprocal && make

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration - GPU-aligned for tensor core optimization
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=8
CONFIG_GPT2_MAX_ITERS=10400
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=n
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters - GPU-optimized for A10G 24GB with tensor core alignment
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration for RATIO ablation
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=128
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.3"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
CONFIG_RA_MLA_USE_FLASH=n
CONFIG_RA_MLA_LOG_METRICS=n
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
CONFIG_RA_MLA_USE_ROPE=n

# Enable ablation study mode - ONLY reciprocal MLP mechanism steps
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="3,4,7,10,12,16"

# Disable test matrix mode - ablation mode runs its own test sequence
CONFIG_TEST_MATRIX_MODE=n

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=n
CONFIG_TRACKER_PROJECT="gpt2-ratio-ablation-reciprocal"

# Advanced options
CONFIG_COMPILE_MODEL=n
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Memory Optimizations
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Test execution
CONFIG_TEST_PARALLEL_JOBS=1
CONFIG_OUTPUT_DIR="test_matrix_results_ratio_ablation_reciprocal"

# GPU-Aligned Dimensions (all multiples of 64 for tensor core optimization):
# - d_model=768=12×64 ✓
# - head_dim=64=1×64 ✓ (optimal for tensor cores)
# - latent_dim=128=2×64 ✓
# - mlp_dim (step 3): 3264=51×64 ✓
# - mlp_dim (step 4,7): 3072=48×64 ✓
# - mlp_dim (step 10,12,16): 2048=32×64 ✓
#
# Reciprocal MLP mechanism steps (6 total):
# - Step 3: Golden ratio + MLP gating 15% (mlp_dim=3264)
# - Step 4: Step 3 + cross-token 10% (mlp_dim=3072)
# - Step 7: RA + golden ratio + mechanisms (mlp_dim=3072)
# - Step 10: MLA + golden ratio + mechanisms (mlp_dim=2048)
# - Step 12: RA + MLA + golden ratio + mechanisms (mlp_dim=2048)
# - Step 16: Full RATIO (all mechanisms: RA+MLA+reciprocal+RA-CT)
