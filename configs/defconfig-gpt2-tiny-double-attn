#
# GPT-2 Tiny with Double Attention (6L/512d/8H)
#
# Double attention configuration: computes both Q@K.T and K@Q.T per layer.
# Uses 2x attention FLOPs vs baseline, but model is scaled down to maintain
# fair compute comparison (6 layers vs 12, d_model=512 vs 768).
#

# Auto hyperparameters
CONFIG_HYPER_PARAM_AUTO=y

# Model: GPT-2 Tiny with Double Attention
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_GPT2_MODEL_NAME="gpt2-tiny"
CONFIG_ENABLE_DOUBLE_ATTENTION=y

# Dataset
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_BLOCK_SIZE=1024

# Training
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_MAX_ITERS=10000
CONFIG_GPT2_MAX_TIME=14400
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"

# Reproducibility
CONFIG_DOUBLE_ATTENTION_SEED=42

# Evaluation
CONFIG_INFERENCE_BENCHMARK=y
CONFIG_RUN_LM_EVAL=y
CONFIG_LM_EVAL_TASKS="hellaswag,arc_easy,winogrande"
CONFIG_LM_EVAL_LIMIT=100

# Optimizer
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_ADV_WEIGHT_DECAY="0.01"
CONFIG_ADV_USE_AMSGRAD=y

# Advanced
CONFIG_GPT2_FLASH_ATTENTION=y
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-tiny-double-attn"
