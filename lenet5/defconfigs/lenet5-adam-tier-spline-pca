# LeNet-5 with Spline-PCA Tokenization + Tiering (Phase 2.3)
# Run with: make defconfig-lenet5-adam-tier-spline-pca && make DEVICE=cpu
#
# Tests advanced spline-PCA tokenization with trajectory tracking
# and hierarchical memory tiering. This is Phase 2.3 of the
# spline-PCA roadmap.
#
# What this does:
# - Compresses MNIST images: 784 dims → 64 PCA components (12x)
# - Tracks component evolution during training (trajectories)
# - Fits cubic splines to trajectories (8 control points)
# - Tiers by BOTH spatial (variance) AND temporal (update freq)
#
# Spatial hierarchy (PCA variance):
# - High-variance components (digit shape) → hot tier candidates
# - Low-variance components (noise/style) → cold tier candidates
#
# Temporal hierarchy (spline trajectories):
# - Stable components (converged early) → can offload
# - Active components (still updating) → keep hot
#
# Purpose: Validate that temporal tiering complements spatial
# tiering. Components with high variance but stable trajectories
# could be offloaded, while low-variance but active components
# should stay hot. Combined scoring provides best tiering.
#
# After training, generates tier_hints_lenet5_spline.json with
# multi-modal tier assignments and saves fitted tokenizer with
# spline representations to lenet5_tokenizer_spline.pkl.
#
# Expected results:
# - Accuracy: ~99.0-99.2% (similar to PCA-only)
# - Training time: ~55-60 min (slight overhead from spline fitting)
# - Some components: High variance but stable → can tier down
# - Other components: Low variance but active → must stay hot
# - Combined tiering: Better than variance-only or temporal-only

# Enable LeNet-5 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_LENET5=y
CONFIG_MODEL_LENET5=y
CONFIG_MODEL="lenet5"
CONFIG_LENET5_DATASET_MNIST=y

# Use AdamW optimizer (needed for Adam state analysis)
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMW=y
CONFIG_OPTIMIZER="adamw"
CONFIG_ADAMW_BETA1="0.9"
CONFIG_ADAMW_BETA2="0.999"
CONFIG_ADAMW_EPSILON="1e-8"
CONFIG_ADAMW_WEIGHT_DECAY="0.0001"

# No pruning for baseline
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# Hyperparameter auto-detection (see docs/hyperparameter-auto-detection.md)
# Auto-detects CPU and selects appropriate batch size
# LeNet-5 uses 4x scale factor for larger batches when memory permits
CONFIG_HYPER_PARAM_AUTO=y
CONFIG_TARGET_EFFECTIVE_BATCH=512
CONFIG_COMPILE_AUTO=y

# Training configuration - CPU optimized
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="0.001"
CONFIG_NUM_WORKERS=4
CONFIG_DEVICE="cpu"

# Disable GPU-specific features for CPU training
CONFIG_COMPILE_MODEL=n
CONFIG_MIXED_PRECISION=n
CONFIG_GPU_WARMUP=n
CONFIG_PIN_MEMORY=n
CONFIG_PERSISTENT_WORKERS=y

# Spline-PCA Tokenization (Phase 2.3)
CONFIG_LENET5_ENABLE_TOKENIZER=y
CONFIG_LENET5_TOKENIZER_SPLINE_PCA=y
CONFIG_LENET5_TOKENIZER_METHOD="spline-pca"
CONFIG_LENET5_PCA_COMPONENTS=64
CONFIG_LENET5_PCA_WHITEN=n
CONFIG_LENET5_SPLINE_CONTROL_POINTS=8
CONFIG_LENET5_TOKENIZER_SAVE_PATH="lenet5_tokenizer_spline.pkl"

# Hierarchical Memory Tiering - Emulated Mode
CONFIG_ENABLE_HIERARCHICAL_TIERING=y
CONFIG_TIERING_ADAM_STATE=y
CONFIG_TIERING_EMULATED=y
CONFIG_TIERING_GENERATE_JSON=y
CONFIG_TIERING_JSON_OUTPUT="tier_hints_lenet5_spline.json"
# Conservative thresholds: 30% hot (L1/L2), 50% warm (RAM), 20% cold (SSD)
CONFIG_TIERING_HBM_THRESHOLD="0.3"
CONFIG_TIERING_CPU_THRESHOLD="0.5"
CONFIG_TIERING_INFERENCE_BENCHMARK=n

# Experiment Tracking (W&B and Trackio)
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="lenet5-tiering"

# Logging
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000
CONFIG_VERBOSE=y

# Test execution (single training run)
CONFIG_TEST_MATRIX_MODE=n
CONFIG_OUTPUT_DIR="lenet5_tier_spline_pca"
