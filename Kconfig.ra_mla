# SPDX-License-Identifier: MIT
# Reciprocal Attention (RA) + Multi-head Latent Attention (MLA) Configuration
# Experimental R&D feature - see docs/ra.md for details

menu "Reciprocal Attention (RA) + MLA (Experimental)"
	depends on MODEL_GPT2

config ENABLE_RA_MLA
	bool "Enable Reciprocal Attention + MLA (EXPERIMENTAL)"
	default n
	help
	  Enable experimental Reciprocal Attention with Multi-head Latent Attention.

	  This is a research R&D feature that reduces attention complexity from
	  O(n²·D) to O(n²·L) where L << D through low-rank KV compression.

	  Status: Implementation complete, validation in progress
	  See: docs/ra.md for full documentation and test roadmap

	  WARNING: This is experimental and unproven. Quality impact unknown.
	  Only use for research exploration, not production training.

if ENABLE_RA_MLA

config RA_MLA_LATENT_DIM
	int "Latent dimension for K/V compression"
	default 64
	range 16 512
	help
	  Latent dimension L for compressing keys and values.
	  Standard GPT-2 has D=64 per head, E=768 total.

	  Compression ratio = D / L:
	    L=32:  12× compression (high risk, max speed)
	    L=64:  6× compression (balanced, recommended)
	    L=128: 3× compression (safe, less benefit)
	    L=256: 1.5× compression (minimal risk, minimal benefit)

	  Lower L = faster but may hurt quality (UNTESTED!)
	  Higher L = safer but less speedup

	  Recommended: 64 for initial testing

config RA_MLA_RA_WINDOW
	int "Reciprocal attention window size"
	default 64
	range 16 512
	help
	  Width of local band for reciprocal symmetric attention.

	  Reciprocal scoring is applied within |i-j| <= W:
	    W=32:  Narrow local context
	    W=64:  Moderate local context (recommended)
	    W=128: Wide local context
	    W=256: Very wide context (expensive)

	  Larger W = more reciprocal computation but potentially better
	  Smaller W = faster but may miss long-range reciprocal patterns

	  Recommended: 64 for initial testing

config RA_MLA_RA_ALPHA
	string "Reciprocal attention weight (alpha)"
	default "0.5"
	help
	  Weight for the reciprocal symmetric term.

	  alpha=0.0: Pure MLA (no reciprocal, safest baseline)
	  alpha=0.5: Balanced reciprocal + standard (recommended)
	  alpha=1.0: Full reciprocal weight

	  Set to "0.0" to test pure MLA first before enabling RA.

	  Recommended: Start with "0.0" to validate MLA, then try "0.5"

config RA_MLA_PER_HEAD_Q_LATENT
	bool "Use per-head Q-to-latent projections"
	default y
	help
	  Whether to use separate Q-to-latent projections for each attention head.

	  Enabled (y): More expressive, more parameters
	    - Each head learns its own Q-to-latent mapping
	    - Better for complex tasks
	    - Parameters: H × D × L (e.g., 12 × 64 × 64 = 49K per layer)

	  Disabled (n): Shared projection across heads
	    - All heads use same Q-to-latent mapping
	    - Fewer parameters, faster
	    - Parameters: E × L (e.g., 768 × 64 = 49K per layer)

	  Recommended: Enable for maximum expressiveness

config RA_MLA_PER_HEAD_V_UP
	bool "Use per-head V up-projections"
	default y
	help
	  Whether to use separate V up-projections for each attention head.

	  Enabled (y): More expressive, more parameters
	    - Each head expands latent V to head space independently
	    - Parameters: H × L × D (e.g., 12 × 64 × 64 = 49K per layer)

	  Disabled (n): Shared V up-projection
	    - All heads use same expansion
	    - Fewer parameters, faster
	    - Parameters: L × D (e.g., 64 × 64 = 4K per layer)

	  Recommended: Enable for maximum expressiveness

config RA_MLA_USE_FLASH
	bool "Enable FlashAttention-style tiling"
	default n
	help
	  Use tiled implementation to reduce memory usage.

	  Enabled (y):
	    - Memory: O(n·L) instead of O(n²·H)
	    - 30-50% memory reduction expected
	    - Currently slower in pure PyTorch (no Triton kernel yet)
	    - No caching support yet (incompatible with generation)

	  Disabled (n):
	    - Standard implementation
	    - Full O(n²·H) memory for attention matrices
	    - Supports caching for generation
	    - Simpler, easier to debug

	  Recommended: Disable until Triton kernel is implemented
	  For training only, can enable to test memory savings.

config RA_MLA_LOG_METRICS
	bool "Log attention metrics (entropy, reciprocity)"
	default y
	help
	  Enable logging of attention-specific metrics:
	    - Attention entropy: measures distribution uniformity
	    - Reciprocity score: correlation between A[i,j] and A[j,i]

	  These metrics are useful for research analysis to understand
	  how RA+MLA differs from standard attention.

	  Minimal performance overhead (<1%).

	  Recommended: Enable for research experiments

config RA_MLA_CACHE_Q_WINDOW
	bool "Cache queries for reciprocal attention at inference"
	default y
	depends on !RA_MLA_USE_FLASH
	help
	  Cache a window of queries for computing reciprocal scores during
	  autoregressive generation.

	  Enabled (y):
	    - Stores last W queries (where W = RA_MLA_RA_WINDOW)
	    - Enables full RA during generation
	    - Memory overhead: O(W·H·D) per layer

	  Disabled (n):
	    - No query caching
	    - Reciprocal term only uses current step's query
	    - Less accurate but lower memory

	  Not available with FlashAttention tiling.

	  Recommended: Enable for generation quality

config RA_MLA_USE_ROPE
	bool "Use RoPE (Rotary Positional Embeddings)"
	default n
	help
	  Apply rotary positional embeddings to latent keys.

	  GPT-2 vanilla doesn't use RoPE (uses learned absolute positions).
	  Enable this only if you've retrofitted RoPE to your GPT-2 model.

	  Recommended: Disable (GPT-2 standard)

config RA_V5_PER_HEAD_GATES
	bool "Use per-head gates for RA (vs per-layer)"
	default n
	help
	  Control gate granularity for RA's w_std and w_rec gates.

	  Per-layer gates (default, RECOMMENDED for small models):
	    - One scalar gate pair (w_std, w_rec) per layer
	    - All heads in a layer share the same standard/reciprocal mixing
	    - Less backward pass overhead (~7% faster per iteration)
	    - Sufficient for GPT-2 class models (12-24 layers)
	    - Rationale: Heads within a layer operate in related geometric
	      subspaces with low spectral diversity. Transformer blocks warp
	      the embedding manifold gradually. Per-head gating introduces
	      backward complexity without capturing meaningfully different
	      geometric behaviors.

	  Per-head gates (enable for large models):
	    - Separate gate pair (w_std, w_rec) for each head
	    - Each head learns independent standard/reciprocal balance
	    - More parameters, slower backward pass
	    - Beneficial for large models (>40 layers) where deeper layers
	      exhibit strong spectral divergence

	  Memory impact:
	    Per-layer:  2 scalars per layer (negligible)
	    Per-head:   2*n_head scalars per layer (e.g., 24 for n_head=12)

	  Recommended: Disable (per-layer) for GPT-2 class models

menu "Reciprocal MLP Mechanisms (Experimental)"
	depends on ENABLE_RA_MLA

comment "Reciprocal MLP extends RA+MLA by enabling bidirectional"
comment "information flow between attention and MLP layers."
comment "Based on scaling-inference.txt: shift compute from attention to MLP"

config RA_MLA_MLP_ATTN_GATE
	bool "Enable Mechanism 1: MLP-to-Attention Gating"
	default n
	help
	  MLP activations modulate attention head weights in the next layer.

	  How it works:
	    - MLP hidden states generate per-head gating weights
	    - These gates modulate attention output in the next layer
	    - Creates feedback loop where MLP influences attention focus

	  Benefits:
	    - MLP can guide attention based on learned features
	    - Enables cross-layer communication
	    - Low computational cost (small gating network)

	  Cost: +0.1-0.2% slowdown, minimal parameter increase

	  Ablation: Disable to test pure MLA+RA baseline
	  Recommended: Enable for full reciprocal MLP

config RA_MLA_MLP_GATE_ALPHA
	string "MLP gating mixing weight (alpha)"
	default "0.1"
	depends on RA_MLA_MLP_ATTN_GATE
	help
	  Weight for mixing gated attention: (1-α)*attn + α*(attn*gate)

	  α=0.0: No gating (disabled)
	  α=0.1: Light gating (recommended)
	  α=0.3: Moderate gating
	  α=0.5: Strong gating

	  Recommended: 0.1 for initial experiments

config RA_MLA_MLP_CROSS_TOKEN
	bool "Enable Mechanism 2: Cross-Token MLP Aggregation"
	default n
	help
	  MLP receives weighted sum of other tokens' activations using
	  attention weights for routing.

	  How it works:
	    - Reuses attention weights from attention layer
	    - Aggregates MLP hidden states across tokens
	    - Enables "attention mass" in MLP space at linear cost

	  Benefits:
	    - MLP gains cross-token context without O(n²) cost
	    - Complements attention's global view
	    - Improves long-range dependencies

	  Cost: +1-2% slowdown, one bmm per MLP layer

	  Ablation: Disable to test MLP without cross-token context
	  Recommended: Enable for full reciprocal MLP

config RA_MLA_MLP_CROSS_ALPHA
	string "Cross-token MLP mixing weight (alpha)"
	default "0.3"
	depends on RA_MLA_MLP_CROSS_TOKEN
	help
	  Weight for mixing cross-token context into MLP hidden state.

	  α=0.0: No cross-token aggregation (disabled)
	  α=0.3: Moderate aggregation (recommended)
	  α=0.5: Strong aggregation
	  α=0.7: Very strong aggregation

	  Recommended: 0.3 for initial experiments

config RA_MLA_MLP_LATENT_RECIP
	bool "Enable Mechanism 3: MLP Latent Space Reciprocity"
	default n
	help
	  Bidirectional information exchange between attention and MLP
	  latent spaces.

	  How it works:
	    - MLP projects to latent space (similar to attention)
	    - Attention latent → MLP (forward path)
	    - MLP latent → Attention (reciprocal path)
	    - Creates symmetric information flow

	  Benefits:
	    - Attention and MLP share latent representations
	    - Enables deeper architectural reciprocity
	    - Aligns with multi-latent compression principles

	  Cost: +2-3% slowdown, small latent projections

	  Ablation: Disable to test without latent-space coupling
	  Recommended: Enable for full reciprocal MLP

config RA_MLA_MLP_RECIP_ALPHA
	string "MLP latent reciprocity mixing weight (alpha)"
	default "0.2"
	depends on RA_MLA_MLP_LATENT_RECIP
	help
	  Weight for mixing latent contributions.

	  α=0.0: No latent reciprocity (disabled)
	  α=0.2: Light reciprocity (recommended)
	  α=0.4: Moderate reciprocity
	  α=0.6: Strong reciprocity

	  Recommended: 0.2 for initial experiments

config RA_MLA_MLP_LATENT_DIM
	int "MLP latent dimension"
	default 128
	range 64 256
	depends on RA_MLA_MLP_LATENT_RECIP
	help
	  Dimension of MLP latent space for mechanism 3.

	  Should be comparable to or larger than RA_MLA_LATENT_DIM
	  to avoid information bottleneck.

	  Values:
	    64: Minimal (matches typical RA_MLA_LATENT_DIM)
	    128: Recommended (2× attention latent)
	    256: Large (4× attention latent, more capacity)

	  Recommended: 128 (2× typical attention latent_dim=64)

config RA_MLA_MLP_GATE_DIM
	int "Gate context vector dimension"
	default 64
	range 32 128
	depends on RA_MLA_MLP_ATTN_GATE
	help
	  Dimension of intermediate context vector for gating network.

	  Smaller = fewer parameters, less expressive
	  Larger = more parameters, more expressive

	  Values:
	    32: Minimal gating capacity
	    64: Recommended
	    128: High gating capacity

	  Recommended: 64 for balanced performance

comment "Parameter Tying & Sparsification (Memory Optimization):"
comment "Reduce 34% memory overhead via parameter sharing and sparse aggregation"

choice
	prompt "Parameter tying mode for MLP-Attention coupling"
	default RA_MLA_MLP_TYING_TIED_TRANSPOSE
	depends on RA_MLA_MLP_LATENT_RECIP
	help
	  Parameter tying mode for bidirectional MLP<->Attention coupling.
	  Similar to ALiBi shared slopes, reduces parameters and improves stability.

config RA_MLA_MLP_TYING_UNTIED
	bool "Untied: Two independent linear maps"
	help
	  Two independent linear maps (A: attn->mlp, B: mlp->attn)
	  Most parameters, most expressive
	  Memory: 2× projections

config RA_MLA_MLP_TYING_TIED_TRANSPOSE
	bool "Tied Transpose: One weight W, use W and W^T (RECOMMENDED)"
	help
	  One weight W, attn->mlp uses W^T, mlp->attn uses W
	  Reduces parameters by 50%, improves training stability
	  Memory: 1× projection (saves 50% vs untied)

	  Recommended for reducing 34% memory overhead

config RA_MLA_MLP_TYING_PER_HEAD_SCALAR
	bool "Per-Head Scalar: Minimal parameters (experimental)"
	help
	  Per-head scalar gates shared across layers
	  Minimal parameters (low degree of freedom)
	  Memory: Only n_heads*2 scalars

	  Most aggressive parameter reduction, experimental

endchoice

config RA_MLA_MLP_TYING_MODE
	string
	default "untied" if RA_MLA_MLP_TYING_UNTIED
	default "tied_transpose" if RA_MLA_MLP_TYING_TIED_TRANSPOSE
	default "per_head_scalar" if RA_MLA_MLP_TYING_PER_HEAD_SCALAR

choice
	prompt "Sparsification mode for cross-token MLP aggregation"
	default RA_MLA_MLP_SPARSE_TOPK
	depends on RA_MLA_MLP_CROSS_TOKEN
	help
	  Sparsification strategy for cross-token MLP aggregation.
	  Reduces MLP token broadcasting overhead significantly.

config RA_MLA_MLP_SPARSE_NONE
	bool "None: Use full attention weights (no sparsification)"
	help
	  Use full attention weights for cross-token aggregation
	  Most accurate but highest memory bandwidth
	  No sparsification overhead reduction

config RA_MLA_MLP_SPARSE_TOPK
	bool "Top-K: Keep only top k tokens per position (RECOMMENDED)"
	help
	  Keep only top-k attention weights per token
	  Reduces broadcasting overhead significantly
	  Recommended: k=8 provides good accuracy/speed tradeoff

	  Expected: 50-75% reduction in MLP aggregation overhead

config RA_MLA_MLP_SPARSE_RMS
	bool "RMS Threshold: Keep tokens above RMS threshold"
	help
	  Keep attention weights above tau * RMS(row)
	  Adaptive sparsification based on attention strength
	  May vary sparsity per token (more flexible than top-k)

endchoice

config RA_MLA_MLP_SPARSE_MODE
	string
	default "none" if RA_MLA_MLP_SPARSE_NONE
	default "topk" if RA_MLA_MLP_SPARSE_TOPK
	default "rms" if RA_MLA_MLP_SPARSE_RMS

config RA_MLA_MLP_SPARSE_K
	int "Top-k value for sparse cross-token aggregation"
	default 8
	range 4 32
	depends on RA_MLA_MLP_SPARSE_TOPK
	help
	  Number of top tokens to keep per position in cross-token MLP.

	  Lower k = more aggressive sparsification, faster, may lose accuracy
	  Higher k = less aggressive, more accurate, slower

	  Values:
	    4: Very aggressive (50% of default)
	    8: Recommended (good balance)
	    16: Conservative (less speedup)
	    32: Minimal sparsification

	  Recommended: 8 tokens

config RA_MLA_MLP_SPARSE_RMS_THRESHOLD
	string "RMS threshold tau for sparse aggregation"
	default "0.5"
	depends on RA_MLA_MLP_SPARSE_RMS
	help
	  Threshold multiplier for RMS-based sparsification.
	  Keep weights where w >= tau * RMS(row)

	  Lower tau = keep more tokens (less sparse)
	  Higher tau = keep fewer tokens (more sparse)

	  Values:
	    0.3: Conservative (keep more tokens)
	    0.5: Recommended (balanced)
	    0.7: Aggressive (sparser)

	  Recommended: 0.5

config RA_MLA_MLP_SPARSE_NORMALIZE
	bool "Normalize sparsified weights"
	default y
	depends on RA_MLA_MLP_CROSS_TOKEN && !RA_MLA_MLP_SPARSE_NONE
	help
	  Re-normalize attention weights after sparsification.

	  Enabled (y): Weights sum to 1.0 after sparsification
	    - Maintains proper probability distribution
	    - More numerically stable
	    - Recommended

	  Disabled (n): Use raw sparsified weights
	    - May not sum to 1.0
	    - Faster but less stable

	  Recommended: Enable for stability

config RA_MLA_MLP_SPARSE_HEAD_AVERAGE
	bool "Average attention weights across heads for cross-token MLP"
	default y
	depends on RA_MLA_MLP_CROSS_TOKEN
	help
	  Whether to average attention weights across heads before aggregation.

	  Enabled (y): Mean over all heads [B,H,T,T] -> [B,T,T]
	    - Simpler, more stable
	    - Recommended

	  Disabled (n): Use alternative aggregation (e.g., max)
	    - More complex
	    - May be less stable

	  Recommended: Enable (use mean)

comment "Ablation Study Guidelines:"
comment "Baseline (no reciprocal MLP): Disable all three mechanisms"
comment "Test Mechanism 1 only: Enable MLP_ATTN_GATE"
comment "Test Mechanisms 1+2: Enable MLP_ATTN_GATE + MLP_CROSS_TOKEN"
comment "Full reciprocal MLP: Enable all three mechanisms"
comment ""
comment "Expected improvements (vs MLA-only baseline):"
comment "Mechanism 1: -0.05 to -0.08 val_loss"
comment "Mechanism 2: -0.10 to -0.15 val_loss (strongest)"
comment "Mechanism 3: -0.05 to -0.10 val_loss"
comment "All three: -0.20 to -0.30 val_loss (cumulative)"

endmenu # Reciprocal MLP Mechanisms

menu "Bitter-Scale Pruning (Inference-Optimized)"
	depends on ENABLE_RA_MLA

comment "Bitter-scale: Structure-aware head pruning targeting KV cache reduction"
comment "Directly operationalizes scaling-inference insights"
comment "Prune attention aggressively, expand MLP capacity"

config RA_MLA_BITTER_SCALE
	bool "Enable Bitter-Scale Pruning"
	default n
	help
	  Enable bitter-scale pruning: structure-aware attention head pruning
	  optimized for inference efficiency.

	  Design philosophy:
	    1. Favor MLP over attention at fixed params (scaling law)
	    2. Prune for KV bytes, not just parameter count
	    3. Multi-signal scoring: RA/MLA usage + attention mass + KV bytes
	    4. State-aligned pruning: properly clean up Adam optimizer states
	    5. Post-prune LR multipliers: lower attention, higher MLP

	  Why "bitter"? Leans into the bitter lesson: simple signals measured
	  relentlessly, with structure-aware pruning targeting IO/KV heavy parts.

	  This is a NEW bitter variant (Bitter 4+) that explicitly targets
	  inference efficiency rather than just parameter reduction.

	  Expected benefits:
	    - 4-8× smaller KV cache (combine MLA latent + head pruning)
	    - 15-20% faster inference despite wider MLPs
	    - Maintain or improve accuracy by shifting capacity to MLPs

	  Cost: +5-10% training overhead for signal collection

	  Recommended: Enable after validating MLA+Reciprocal MLP baseline

config RA_MLA_BITTER_PRUNE_INTERVAL
	int "Pruning interval (iterations)"
	default 1000
	range 100 5000
	depends on RA_MLA_BITTER_SCALE
	help
	  How often to evaluate and potentially prune heads (in iterations).

	  More frequent pruning:
	    - Responds faster to training dynamics
	    - Higher computational overhead
	    - May be too aggressive

	  Less frequent pruning:
	    - More stable, lets model adapt
	    - Lower overhead
	    - May miss optimal pruning opportunities

	  Recommended: 1000 iterations (balance stability and responsiveness)

config RA_MLA_BITTER_TARGET_HEADS
	int "Target number of heads to keep"
	default 8
	range 4 12
	depends on RA_MLA_BITTER_SCALE
	help
	  Target number of attention heads to keep after pruning.
	  GPT-2 starts with 12 heads.

	  Aggressive pruning (4-6 heads):
	    - Maximum KV cache reduction
	    - Requires strong reciprocal MLP to compensate
	    - Higher risk

	  Moderate pruning (7-9 heads):
	    - Good balance of efficiency and capacity
	    - Recommended starting point

	  Conservative pruning (10-11 heads):
	    - Minimal risk, modest gains
	    - Good for validation

	  Recommended: 8 heads (33% reduction, 1.5× KV cache compression)

config RA_MLA_BITTER_WEIGHT_RA
	string "Weight for RA/MLA usage signal"
	default "1.0"
	depends on RA_MLA_BITTER_SCALE
	help
	  Weight for RA/MLA usage score in multi-signal importance.

	  Score = w_ra*ra + w_attn*attn - w_kv*kv_bytes

	  Higher weight → prioritize heads with high RA/MLA participation
	  Lower weight → reduce influence of RA signals

	  Recommended: 1.0 (equal weight with attention mass)

config RA_MLA_BITTER_WEIGHT_ATTN
	string "Weight for attention mass signal"
	default "1.0"
	depends on RA_MLA_BITTER_SCALE
	help
	  Weight for attention mass (how much each head is used).

	  Attention mass measures aggregate attention probability per head.
	  High mass → head is actively routing information.

	  Recommended: 1.0 (equal weight with RA signals)

config RA_MLA_BITTER_WEIGHT_KVBYTES
	string "Weight for KV bytes penalty"
	default "0.5"
	depends on RA_MLA_BITTER_SCALE
	help
	  Weight for KV bytes in importance score (NEGATIVE term).

	  Score = w_ra*ra + w_attn*attn - w_kv*kv_bytes

	  Higher weight → more aggressive KV cache optimization
	  Lower weight → prioritize usage over KV savings

	  The negative term means: prune heads that save the most KV bytes
	  (if their usage doesn't justify the KV cost).

	  Recommended: 0.5 (balance usage and KV efficiency)

config RA_MLA_BITTER_LR_SCALE_ATTN
	string "Post-prune LR scale for attention"
	default "0.9"
	depends on RA_MLA_BITTER_SCALE
	help
	  Learning rate multiplier for attention parameters after pruning.

	  After pruning, remaining heads are high-value. Use conservative
	  LR to preserve what's important.

	  Values:
	    0.8: Very conservative (slow adaptation)
	    0.9: Recommended (stable but responsive)
	    1.0: No change (standard LR)

	  Recommended: 0.9 (slightly lower for stability)

config RA_MLA_BITTER_LR_SCALE_MLP
	string "Post-prune LR scale for MLP"
	default "1.1"
	depends on RA_MLA_BITTER_SCALE
	help
	  Learning rate multiplier for MLP parameters after pruning.

	  After pruning attention, encourage MLP to expand into freed capacity.
	  Higher LR helps MLP adapt faster.

	  Values:
	    1.0: No change (standard LR)
	    1.1: Recommended (gentle encouragement)
	    1.2: Aggressive (faster MLP growth)

	  Recommended: 1.1 (encourage MLP expansion)

config RA_MLA_BITTER_EMA_BETA
	string "EMA beta for signal smoothing"
	default "0.99"
	depends on RA_MLA_BITTER_SCALE
	help
	  Exponential moving average beta for smoothing importance signals.

	  Higher beta (closer to 1.0):
	    - Smoother, more stable signals
	    - Slower response to changes
	    - Less noise

	  Lower beta (closer to 0.9):
	    - Faster response to dynamics
	    - More noise, less stable
	    - May prune prematurely

	  Recommended: 0.99 (very smooth, stable pruning decisions)

comment "Integration with Reciprocal MLP:"
comment "Bitter-scale and reciprocal MLP share the same principle:"
comment "Compress attention aggressively, expand MLP capacity"
comment ""
comment "Expected combined effect:"
comment "MLA latent compression: 6× KV reduction"
comment "Bitter-scale head pruning: 1.5× KV reduction (12→8 heads)"
comment "Combined: 9× smaller KV cache"
comment "MLP expansion: 3072 → 4096-5120 hidden"
comment "Result: Faster inference + better accuracy"

endmenu # Bitter-Scale Pruning

menu "Reciprocal MLP Ablation Study (Multi-Run)"
	depends on ENABLE_RA_MLA

comment "Test multiple reciprocal MLP configurations in one run"
comment "Enables systematic ablation study of mechanisms 1, 2, and 3"

config RA_MLA_ABLATION_MODE
	bool "Enable RATIO Ablation Study Mode"
	default n
	help
	  Enable RATIO ablation study mode with 15 steps testing golden ratio
	  (1:2.5), MLP mechanisms, RA, MLA, and structure-aware optimization.

	  Steps 0-14 test isolated variables:
	    0. Baseline GPT-2 (ratio 1:2.0, standard attention)
	    1. Baseline + SPAM pruning 50% (pruning baseline)
	    2. Golden ratio 1:2.5 via MLP resize (tests ratio alone)
	    3. Step 2 + MLP gating 15%
	    4. Step 3 + cross-token 10%
	    5. Baseline + RA (ra_alpha=0.3, ratio 1:2.0)
	    6. RA + golden ratio 1:2.5
	    7. Step 6 + mechanisms
	    8. Baseline + MLA (latent_dim=128, ratio 1:3.0)
	    9. MLA + golden ratio 1:2.5
	   10. Step 9 + mechanisms
	   11. RA + MLA + golden ratio 1:2.5
	   12. Step 11 + mechanisms
	   13. Step 10 + AdamWStructure
	   14. Step 13 + ratio-preserving pruning 50%

	  Victory condition: Step 14 > Step 1 (RATIO beats SPAM pruning)

	  To run all steps: set ABLATION_STEPS="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14"
	  To run subset: set ABLATION_STEPS="0,1,2,8,9,14"

config RA_MLA_ABLATION_BASELINE
	bool "Include baseline (MLA-only)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run baseline configuration (MLA-only, no reciprocal MLP).
	  Expected: val_loss ~3.61, ppl ~37.2

config RA_MLA_ABLATION_STEP1
	bool "Include step 1 (Mechanism 1 only)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run mechanism 1 only: MLP-to-Attention Gating.
	  Expected: -0.02 to -0.05 improvement over baseline.

config RA_MLA_ABLATION_STEP2
	bool "Include step 2 (Mechanisms 1+2)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run mechanisms 1+2: MLP gates + Cross-token MLP.
	  Expected: -0.05 to -0.10 improvement over baseline.
	  Predicted strongest contributor.

config RA_MLA_ABLATION_STEP3
	bool "Include step 3 (All mechanisms)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run all three mechanisms: Full reciprocal MLP.
	  Expected: -0.10 to -0.15 improvement over baseline.

config RA_MLA_ABLATION_STEP4
	bool "Include step 4 (Mechanisms 1+2, reproducibility check)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run mechanisms 1+2 (reproducibility check for step 2).

	  This step is identical to step 2. It was originally intended
	  for comparing optimizers (AdamW vs AdamWSPAM), but since the
	  default config uses AdamWSPAM for all steps, this serves as
	  a reproducibility verification.

	  Expected: Should match step 2 results exactly.

config RA_MLA_ABLATION_STEP5
	bool "Include step 5 (Baseline + RA)"
	default n
	depends on RA_MLA_ABLATION_MODE
	help
	  Run baseline with reciprocal attention (ra_alpha=0.3, ratio 1:2.0).
	  Tests reciprocal attention scoring without golden ratio or MLA.

config RA_MLA_ABLATION_STEP6
	bool "Include step 6 (RA + golden ratio)"
	default n
	depends on RA_MLA_ABLATION_MODE
	help
	  Run RA with golden ratio (ra_alpha=0.3, mlp_dim=3840, ratio 1:2.5).
	  Tests if golden ratio helps reciprocal attention.

config RA_MLA_ABLATION_STEP7
	bool "Include step 7 (RA + ratio + mechanisms)"
	default n
	depends on RA_MLA_ABLATION_MODE
	help
	  Run RA + golden ratio + MLP mechanisms (gating + cross-token).
	  Tests if RA enhances mechanisms vs standard attention (step 4).

config RA_MLA_ABLATION_STEP8
	bool "Include step 8 (Baseline + MLA)"
	default n
	depends on RA_MLA_ABLATION_MODE
	help
	  Run baseline with MLA (latent_dim=128, ratio 1:3.0).
	  Tests MLA alone without golden ratio adjustment.

config RA_MLA_ABLATION_STEP9
	bool "Include step 9 (MLA + golden ratio)"
	default n
	depends on RA_MLA_ABLATION_MODE
	help
	  Run MLA with golden ratio (latent_dim=128, mlp_dim=2560, ratio 1:2.5).
	  Tests if adding golden ratio to MLA helps.

config RA_MLA_ABLATION_STEP10
	bool "Include step 10 (MLA + ratio + mechanisms)"
	default n
	depends on RA_MLA_ABLATION_MODE
	help
	  Run MLA + golden ratio + MLP mechanisms.
	  Tests if mechanisms add value on top of MLA + ratio.

config RA_MLA_ABLATION_STEP11
	bool "Include step 11 (RA + MLA + golden ratio)"
	default n
	depends on RA_MLA_ABLATION_MODE
	help
	  Run RA + MLA + golden ratio (ra_alpha=0.3, latent_dim=128, ratio 1:2.5).
	  Tests combining all attention strategies with golden ratio.

config RA_MLA_ABLATION_STEP12
	bool "Include step 12 (RA + MLA + ratio + mechanisms)"
	default n
	depends on RA_MLA_ABLATION_MODE
	help
	  Run RA + MLA + golden ratio + MLP mechanisms (full combination).
	  Tests best combination of all innovations.

config RA_MLA_ABLATION_STEP13
	bool "Include step 13 (Step 10 + AdamWStructure)"
	default n
	depends on RA_MLA_ABLATION_MODE
	help
	  Run MLA + ratio + mechanisms with AdamWStructure optimizer.
	  Tests structure-aware optimization with role-specific learning rates.

config RA_MLA_ABLATION_STEP14
	bool "Include step 14 (Full RATIO framework)"
	default n
	depends on RA_MLA_ABLATION_MODE
	help
	  Run step 13 + ratio-preserving pruning (50% sparsity).
	  Full RATIO framework: MLA + ratio + mechanisms + structure-aware + pruning.
	  Victory condition: Should beat step 1 (SPAM pruning baseline).

config RA_MLA_ABLATION_STEPS
	string "Ablation steps to run"
	default "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14"
	depends on RA_MLA_ABLATION_MODE
	help
	  Comma-separated list of ablation steps to run.
	  Format: "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14"

	  0 = Baseline GPT-2 (ratio 1:2.0)
	  1 = Baseline + SPAM pruning 50%
	  2 = Golden ratio 1:2.5 via MLP resize
	  3 = Step 2 + MLP gating 15%
	  4 = Step 3 + cross-token 10%
	  5 = Baseline + RA (ra_alpha=0.3)
	  6 = RA + golden ratio 1:2.5
	  7 = Step 6 + mechanisms
	  8 = Baseline + MLA (ratio 1:3.0)
	  9 = MLA + golden ratio 1:2.5
	  10 = Step 9 + mechanisms
	  11 = RA + MLA + golden ratio
	  12 = Step 11 + mechanisms
	  13 = Step 10 + AdamWStructure
	  14 = Step 13 + ratio-preserving pruning

	  You can manually override to run a subset, e.g.:
	  "0,1,2,8,9,14" - Test baseline, golden ratio, MLA, and full RATIO
	  "0,2,4,10,14" - Quick test of key milestones

endmenu # Reciprocal MLP Ablation Study

endif # ENABLE_RA_MLA

endmenu
